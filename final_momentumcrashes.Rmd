---
title: "Momentum Crashes"
output:
  pdf_document: default
  'html_document:': defaultv
  html_document: default
editor_options:
  chunk_output_type: inline
---

#
# NOTE TO READER 
# 
# This dataset has survivorship biases in it! As we we dont take into considiration when stocks are leaving or not
# Remember to clean ram and restart R when running the code to avoid errors each time its run.

```{r packloading}
library(dplyr)
library(tidyquant)
library(tidyverse)
library(plyr)
```
#
# If used on SP500, remember to use line 56 instead of line 54. This is becouse of different data structures. 
#

```{r read data}
library(readr)

df = read_delim("db_dat/sp500.csv",  ";", escape_double = FALSE, locale = locale(decimal_mark = ","), na = "NA", trim_ws = TRUE)

# MSCI CANADA DOES NOT WORK 
# Tickers to implement
# ^STOXX Euro stoxx 600 SHIT DATA SCRAP IT !!!!
# ^GSPC = SP500 WORKS
# ^AXJO = Australian SP 200 WORKS
# ^N225 = Nikkei 225 Japan,ERRORS IN NIKKEI BAD DATASET! 
# ^HSI = HSI 225 Hong Kong,ERRORS IN HSI BAD DATASET! 

index_use = c("^GSPC") # Select the index to load here 

names(df)[1]  = "date" # Change the name of col 2 
names(df)[2]  = "symbol" # Change the name of col 2 
names(df)[3]  = "tot_ret" # Change the name of col 2 
names(df)[4]  = "mcap" # Change the name of col 2 
names(df)[5]  = "close" # Change the name of col 2 

df$date =  as.Date(df$date, format = "%Y-%m-%d")
df$symbol = df$symbol %>% as.character() 

df$tot_ret = as.numeric(gsub(",",".",df$tot_ret,fixed=TRUE))

#df$mcap = gsub('\\s+', '', df$mcap) %>% as.numeric() # Use this when using all other datasets.

df$mcap = as.numeric(gsub(",",".",df$mcap,fixed=TRUE)) # ONLY USE THIS IF SP500 dataset, this is becouse the input is different

df$close = as.numeric(gsub(",",".",df$close,fixed=TRUE))

df[is.na(df)] = 0 # Set NAs to zero 

devtools::unload("readr") #unload the pack
```

```{r prepare data}
library(bsts)
##
## Cleaning some data, removing some dates
##
df$date =  as.Date(df$date, format = '%d.%m.%Y') # Setting the date to standard format 

portstart_date = c("2005-01-01") # Portfolio start from the dataset
start_year = 2005 # Important to set this equal to top ! Will be used further down

df = dplyr::filter(df, date > portstart_date) # Filter the dates 

df = df %>% dplyr::filter(close != 0) %>% dplyr::filter(mcap != 0) %>% dplyr::select(-(tot_ret)) #MCAP and close price cannot be zero 

transform(df, date = bsts::LastDayInMonth(df$date)) # Changeing the dates to the last day in the month

df = df %>% group_by(symbol) %>% arrange(date)  # Group and arrange 

devtools::unload("bsts") # unload pack 
```

```{r return calc pers stock}
# Return calculation for each stock in. More accurate then the "simple" lead - var /var method 
# Normal methods gives high random scores, as it tries to calculate with next col
# This method returns NA when there is no other date to calculate to (no next lead)

return_calculations = function(portfolio){
  reb_month =  unique(portfolio$date) # Find all unique stocks 
  reb_stock = unique(portfolio$symbol) # Find all unique dates 
  temp_holder = NULL # working variable 
  temp_holder2 = NULL # Working variable 
  pb = txtProgressBar(min = 0, max = length(reb_stock), initial = 0, style = 3) # Progressbar 
  count = 1 # Counter to see time remaining
    for (j in 1:length(reb_stock)){ # For each iteration of all stocks  do this 
      temp_holder = portfolio %>% dplyr::filter(symbol == reb_stock[j]) # Filter to calculate pr stock 
      temp_holder$return  = ((lead(temp_holder$close) - temp_holder$close)  / temp_holder$close) # Return calculations
      temp_holder2 = rbind(temp_holder,temp_holder2)  # Bind it together with a empty var then add more when it comes 
      count = count + 1 # increase the counter for the function
      vv = setTxtProgressBar(pb,count) # set the progressbar to update 
    }
  return(temp_holder2) # Return the entire table with returns 
}  

df = return_calculations(df) # Runs function
df = df %>% dplyr::filter(date < "2021-10-01") # Last calculations does not have anything above to calculate return from, therefor rem. 
```




```{r 2:12 month return to messure momentum}
# This creates 2:12 cols, these cols will tell you if you sold an asset today, how much would you gain since you bought it 

return_function = function(portfolio,period){ # Defining function to be run in func
  x = ((portfolio$close - lag(portfolio$close, n = period))  / lag(portfolio$close, n = period)) # return with iterations of lag
  return(x) # return
}

mom_2_12= function (portfolio){ # Defining function
  uniq_stocks =  unique(portfolio$symbol) # finds each stock to calc returns
  temp_holder = 0 %>% as.data.frame()
  temp_holder2 = 0 %>% as.data.frame() # These are just working variables 
  temp_holder3 = 0 %>% as.data.frame()
  count = 0
  pb = txtProgressBar(min = 0, max = length(uniq_stocks), initial = 0, style = 3) # progress bar
   z = 0 %>% as.data.frame() # predefine a new datatable to work with 
  for (i in 1:length(uniq_stocks)){ # for each date we have in the portfolio do this
      z = portfolio %>% dplyr::filter(uniq_stocks[i] == symbol) # filter each stock 
      count = count + 1 # Adds 1 to the counter for progressbar 
       setTxtProgressBar(pb,count) # Starting the progress bar
      for(j in 2:12) { # for each stock do this 12 times (1 is skipped per the paper)
        #temp_holder  = momentum(z$return, n=j)
        temp_holder = return_function(z, j) # Call function from above 
        name = paste0("mom", sep = "_", j) # Creates the name mom and pasts j to it, so it becomes mom_2 mom_3 etc
        z[[name]] =  temp_holder # Names the column
        if (j == 12){ 
          temp_holder2  = bind_rows(temp_holder2, z) # at the end of the "run" bind it together for later export
        } 
     }
    }
  return(temp_holder2) # returns the entire updated table 
}  

test  = mom_2_12(df) # Test is just temp name, but this calls the functions from above 
df = test %>% slice(-c(1)) # Due to bind with one Zero, there is a redundant row, this is removed here. 
test = NULL # Put test df to null
```

```{r scrubbing data making som return rankings}
# We can get data that is calculated to inf! this is due to the table sometimes provides us with nothing to calculate to or from \ incase a delesting etc. Therefor we have to convert info to get Zero as a value, if we filter out inf we will remove to many returns! 

library(matrixStats)
library(R.utils)

table_scrubber = function() { # Making a table scrubber function 
  df[7:17] = na_if(df[7:17], 0) # Setting NAs in mom_X columns to zero 
  df[sapply(df, is.infinite)] = NA # Infinate to NA 
  df[sapply(df, is.nan)] = NA # NANs to NA 
} 

#This is done to get clear tables, to use the functions below as we will count real values

#table_scrubber() #Runs the function 

df[7:17] = na_if(df[7:17], FALSE)
x = !isZero(df[7:17])
x = tidyr::replace_na(x, 0)
y = rowSums(x) %>% tibble()
df = cbind(df,y)
colnames(df)[NCOL(df)] = "available_returns"

df$cum_return = matrixStats::rowProds(1+as.matrix(df[7:17]), na.rm=T) %>% log() #Creating cumalative returns as logs 

table_scrubber() # Just makeing sure my data is nice and shiny 
y = NULL
devtools::unload("matrixStats") # Unload old pack not needed anymore
devtools::unload("R.utils")
```

```{r actual ranking, over 8 month returns, normalizing values for quantiles}
# Before we input it into quantiles we have to normalize our data, as the function can only recive between 0 and 1. 
# As our data set goes from -1 to 1 

df$cum_return_norm = df$cum_return / df$available_returns #Normalising our returns devided on number on total number of rets
df = df %>% dplyr::filter(available_returns >= 8)# Above 8 monthly returns in ranking is put into the dataset
x = df$cum_return_norm  # Just renaming it to make it easier 
df$norm_norm_cret = (x-min(x))/(max(x)-min(x)) # Now we "normalize" the pre "normalized" returns to pipe it into the next function

x = NULL # Set x to null as its not needed. Sometimes I make diffrent vars for testing purposes, incase i need to check something.
```

```{r putting into monthly deciles}
# Putting the stocks into deciles, ie top performers are in bracket 10 and bot in 1 

decile_seperation = function(portfolio){ #Create funk 
  x = portfolio #%>% dplyr::distinct(norm_norm_cret, .keep_all = T) # Redundant code, as it has problems if there are equal numbers to devide
  reb_month =  unique(x$date) # Finding all unique dates 
  temp_holder1 = NULL
  temp_holder2 = NULL # Empty working var 
  temp_holder3 = NULL # Empty working var 
  temp_holder4 = NULL # Empty working var
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
  count = 1 #counter for usage in progress bar
    for (j in 1:length(reb_month)){ # Takes all months with the mentioned criteria above.
      temp_holder1 = x %>% dplyr::filter(date == reb_month[j]) # filter each dates so we have one specific month
      temp_holder2$quantile = cut(temp_holder1$norm_norm_cret, # put it into dociles, based on monthly return
          quantile((temp_holder1$norm_norm_cret),
                    probs=seq(from=0,to=1,by=1/10), #Here you can change the deciles into quantiles etc if you wish 
                    na.rm =T),  # Defining deciles 
                    include.lowest=TRUE, 
                    labels=FALSE)
       temp_holder3= add_column(temp_holder1 ,as_tibble(temp_holder2)) # adds the decile score to the table 
       temp_holder4= bind_rows(temp_holder3 ,temp_holder4)  # Binds the score and the folder for a stock and date
       count = count + 1 # Adds 1 to the counter for progressbar 
       setTxtProgressBar(pb,count) # Starting the progress bar
    }  
  return(temp_holder4) # Returns from function a entire dataset with quantiles attached.
}

df = as_tibble(decile_seperation(df)) # calls the function on tibble df, and saves as dfa  

names(df)[names(df) == "value"] <- "quantile" # Renames the colume value to quantile.  

```

```{r Monthly value weighted function}
# Creates monthly value weights per stock 

v_weights= function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  temp_holder = 0 %>% as.data.frame() # Working variable 
  z = 0 # Working var 
  count = 0 # setting the counter 
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == date) # Select the first date on the list, filter out the rest
      count = count + 1 # Adds 1 to the counter for progressbar 
      setTxtProgressBar(pb,count) # Starting the progress bar
        for (j in 1:10){ # In every single month from above, do the following in every decile 
          x = z # Working var 
          a = x %>% dplyr::filter(quantile == j) #Filter out each quantile 
          tot_w = sum(a$mcap)  # Sum all the market caps for that decile in a specific dates 
          a$vw  = (a$mcap/tot_w) %>% as.data.frame() # Create individual stock weighting based on market cap 
          temp_holder = bind_rows(a,temp_holder)  # Bind rows to get out put 
          if (j == 1 && i == 1) { # This is only here to remoe the one binding col!
             temp_holder = temp_holder %>% slice(-c(nrow(temp_holder)))
             } 
        }
      
    }
  return(temp_holder)
}  
        
df  = v_weights(df) # Calls the funk updates table 
df = df %>% slice(-c(ncol(df))) # slice of a redundant column 

#This is just to check the that the function sums to one "deeper" into the dataset
dfx = df %>% dplyr::filter(quantile == 1) %>% dplyr::filter(date == "2015-12-31") # Checking that weights sum to one 
sum(dfx$vw) # OK if it sums to one
dfx = NULL # Clears dataframe
```

```{r Making bot and top quabtiles}
# Creating top deciles in own dataframes 
top_1 = df %>% dplyr::filter(quantile == 10) %>% arrange(date) # making the best performance portf
mid_1 = df %>% dplyr::filter(quantile == 5) %>% arrange(date)  # med performance portfolio based 
bot_1 = df %>% dplyr::filter(quantile == 1) %>% arrange(date)  # worst performance portfolio based 
```

```{r retriving market index for index, cleaning and formating, THIS NEEDS TUNING IF CHANGE OF DATES!}
# We need the index, this is downloaded from the internett, then processed to match our data. 
## CREATING A MONTHLY DATASET for index!
##
library(bsts)
library(openair)

from = today() - years(40) #Takes from "today" and retrives 40 years of history
  
index = tq_get(index_use, get = "stock.prices", from = from) %>% #Get index prices (GSPC is index)
    group_by(symbol)


index$date =  as.Date(index$date, format = '%Y-%m-%d') # Changeing date format 

index = dplyr::filter(index, date > portstart_date) # Filters out the dates to match our main dataset 

month_day = LastDayInMonth(index$date) %>% unique() %>% as.Date() # Retrives all last day in month, as we need the monthly data 

index = index %>% group_by(symbol) # group by symbol 

selecter = c("symbol", "date", "close") # Variables to have in top and bot tibble
index = dplyr::select(index,all_of(selecter)) # Select the cols we want to work with 


index_name = index[1,1] # Retrives the index name
index[1] = index_name # Rename all cols in the index to name of the index for uniqueness 

date_filter = function(portfolio) { # Filter function creation. This is to create final day of the month 
  counter = 0
  start_date = portfolio[1,2] # The start date is the first in the dataset
  start_date = start_date$date %>% as.Date(format = '%Y-%m-%d') # ISO standard date 
  end_date = portfolio[nrow(portfolio),2] # defines the end date 
  end_date = end_date$date %>% as.Date(format = '%Y-%m-%d') # Standarize 
  years = seq(from = start_date, to = end_date, by = 'year') # figure out how many years it is between the dates 
  years = length(years) # Finds length of years 
  temp_holder1 = NULL
  temp_holder2 = NULL
  temp_holder3 = NULL
  pb = txtProgressBar(min = 0, max = years, initial = 0, style = 3) # progress bar
  for (i in 1:years) { # For every year in the index do this 
    year_counter = (start_year + counter) ###############HARDCODED 2000 NEEDS TO BE CHANGED ACCORDING TO YEAR! ###########
    x = selectByDate(portfolio, year = year_counter ) # Select each year 
    counter = counter + 1
    setTxtProgressBar(pb,counter) # Starting the progress bar
    for (j in 1:12){
      temp_holder1 = x$date[lubridate::month(x$date) == j]  # Finds when a month is the month of J 
      temp_holder1 = temp_holder1[length(temp_holder1)] # Finds the last day in the month 
      temp_holder3 = x %>% dplyr::filter(x$date == temp_holder1)# Filters out the rest of the days
      temp_holder2 = rbind(temp_holder2,temp_holder3) # Binding them together for output 
    }
      
  }
  return(temp_holder2) # Output
}

index = date_filter(index) # Filters 

index = index %>% arrange(date) # arrange dates again 

index$return = ((lead(index$close) - index$close)  / index$close) # New col with returns 

index[is.na(index)] = 0 # Scrubbing 
index[sapply(index, is.infinite)] = 0 #Scrubbing 

index$cum_ret = cumprod(1 + index$return) # Create cumalative returns based on the index 

index = index %>% dplyr::filter(cum_ret != 0) %>% dplyr::filter(cum_ret != 1) # cannot have 0 or 1 in cumalative return 
# This is due to how we calculate returns, as first month does not have returns to calculate from 

devtools::unload("bsts") 
devtools::unload("openair")
```

```{r cumalative returns for each portfolio calc, warning = FALSE}
###
### Calculates returns on specific portfolios, invest 1 dollar in time Zero and does compounding Value WEIGHTED
###### 
selecter = c("symbol", "date", "return", "vw") # Variables to have in top and bot tibble
top = dplyr::select(top_1,all_of(selecter)) # Selects from selecter
mid = dplyr::select(mid_1,all_of(selecter)) # Selects from selecter
bot = dplyr::select(bot_1,all_of(selecter)) # Selects from selecter


cumalative_ret_portfolio = function (portfolio){ # Defining function
reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
tot_ret = c() # empty vector 
  for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
    if (i == 1){  # First round do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest 
      ret = (1+z$return) * z$vw  # Investing 1 dollar divided by stocks multiplied with returns
      tot_ret[i]  = sum(ret) # Summarize all the returns from investing one dollar 
    }
    if (i > 1){ # When its beyond start date, (t+1). Do the following
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Filter out the dates not needed
      ret = tot_ret[i-1] * z$vw * (1+z$return) #Returns * the weights 
      tot_ret[i]  = sum(ret) # Sum it all up to reinvest 
    }
  }
return(tot_ret) #returns the calculations out of the loop
}

##
## This section retrives a string of numbers (return per rebalancing period)
##

top_score = cumalative_ret_portfolio(top) # runs the script on top portfolio 
mid_score = cumalative_ret_portfolio(mid) # runs on bot portf.
bot_score = cumalative_ret_portfolio(bot) # runs on bot portf.
```

```{r WML weighting and calculations}
#creating wml portfolio
wml_weights= function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  temp_holder = 0 %>% as.data.frame()
  z = 0 
  count = 0 
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == date) # Select the first date on the list, filter out the rest
      count = count + 1 # Adds 1 to the counter for progressbar 
      setTxtProgressBar(pb,count) # Starting the progress bar
          x = z
          tot_w = sum(x$mcap)  
          x$vw  = (x$mcap/tot_w)
          temp_holder = bind_rows(x,temp_holder) 
          if (i == 1) { # This is only here to remoe the one binding col!
             temp_holder = temp_holder %>% slice(-c(nrow(temp_holder)))
             } 
        }
      return(temp_holder)
    }
  
wml_cumalative_ret_portfolio = function (port){ # Defining function
reb_month =  unique(port$date) # finds each re-balancing months from the portfolio
tot_ret = c() # empty vector 
sum_w = 0
temp = 0 
  for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
    if (i == 1){  # First round do this 
      z = port %>% dplyr::filter(reb_month[i] == port$date) # Select the first date on the list, filter out the rest 
      #z = z %>% dplyr::filter(ret_inv != 0) 
      ret = (1+z$ret_inv) * z$vw  # Investing 1 dollar divided by stocks multiplied with returns
      tot_ret[i]  = sum(ret) # Summarize all the returns from investing one dollar 
    }
    if (i > 1){ # When its beyond start date, (t+1). Do the following
      z = port %>% dplyr::filter(reb_month[i] == port$date) # Filter out the dates not needed
      ret = tot_ret[i-1]* z$vw * (1+z$ret_inv) #Returns * the weights 
      tot_ret[i]  = sum(ret) # Sum it all up to reinvest 
    }
  }
return(tot_ret) #returns the calculations out of the loop
}

bot_1$ret_inv = bot$return * -1   # Inverting returns 
top_1$ret_inv = top$return        # just adding the same on top for easier merging
wml_score = rbind(top_1,bot_1)    # Binding the to together 

wml_score_parent  = wml_weights(wml_score) # finding weights for wml
wml_score = wml_cumalative_ret_portfolio(wml_score_parent)


dfx = wml_score_parent %>% dplyr::filter(date == "2010-12-31") # Checking that weights sum to one 
sum(dfx$vw) # OK if it sums to one
dfx = NULL
```


## NB! WML has bad returns to work with, therefor later on we will use TOP\Botto do the dynamic weighting on

```{r just renaming some cols, and merging tables}
##
# Takes the dates and merges then with the scores, making a more user friendly experience
##
scores = function (portfolio, parent_portfolio){ # makes function
  portfolio = portfolio %>% as_tibble() # converts the score into a tibble for easier processing 
  colnames(portfolio)[1] = "cum_rets"  # Rename the cumalative returns 
  dates = parent_portfolio$date  %>% as.Date() %>% unique() # Find the dates from the parent calulations (pre last function)
  portfolio$date  =  dates  # takes the dates and put them into tbe portfolio
  return(portfolio) # Returns resaults 

} 

selecter = c("symbol", "date", "return", "vw")
wml_score_parent = wml_score_parent %>% dplyr::select(all_of(selecter))

top_score_reb = scores(top_score, top) #Calls the function on the score and the parrent 
mid_score_reb = scores(mid_score, mid) # same but for mid score 
bot_score_reb = scores(bot_score, bot) # same but for bot score  bot_score_inverse
wml_score_reb = scores(wml_score, wml_score_parent)

```

```{r combinding all tables from above baed on date}
# Combinding the data from above into one DF, for easier access 

total_table = function(){ 
  if ((top_score_reb$date == bot_score_reb$date) && (top_score_reb$date == bot_score_reb$date)) {
    total_score = NULL
    total_score$date  = as_tibble(top_score_reb$date)
    total_score$top = top_score_reb$cum_rets
    total_score$mid = mid_score_reb$cum_rets
    total_score$bot = bot_score_reb$cum_rets
    total_score$wml = wml_score_reb$cum_rets
  return(total_score)
  }
}


total_score = as.data.frame(total_table()) # Saves the output as dataframe, so we can translate date to date
names(total_score)[names(total_score) == "value"] <- "date"
total_score[,1] = total_score$date  %>% as.Date() # Converts date to date format
```

```{r importing fama franch data}
# Donwload research data from https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip
# Select monthly dataset. 
# Remember to scrub the data! Add Date as a column (or else it gets indexed) # Remove yearly data at the bottom 
# Remove notes top and bottom on the file, or else you will get strange data. 

ff_data = read.csv(file="F-F_Research_Data_Factors.CSV", sep=",", header = TRUE) # load file in working directory, seperation is , 
ff_data$date = ff_data$date %>%  ym()  #put the date into usable format in tune with our other datasets
ff_data = ff_data %>% dplyr::filter(date > portstart_date)  # Filter out everydate below 1990-09-01

day(ff_data$date) = days_in_month(ff_data$date) # Changing the day in our dataset to match the other data (ie last day of month)
ff_data$date = ff_data$date  %>% as.Date() 

###
### MATCHING DATES AGAIN TO MAKE SURE ITS IN ORDER!!!!! I HATE DATES !!!! 
day(total_score$date) = days_in_month(total_score$date) # to match up with ff_dates
day(total_score$date) = days_in_month(total_score$date)  
day(index$date) = days_in_month(index$date) 
```

```{r, converting FF data to percentages}
# Devide by 100 to get the percentage and not the "percentage" :) As in data 5.2 is turned into 0.052 to match data 
ff_data$ff_mkt_rf = ff_data$Mkt.RF/100
ff_data$ff_smb = ff_data$SMB/100
ff_data$ff_hml = ff_data$HML/100
ff_data$ff_rf = ff_data$RF/100
```

```{r syncing the dates of ff data with other dates }
#
# Take data from ff_data, regarding risk free rate etc. Filter it so we can fit it into the calculated dataset
# Here we match up the dates from the dataset, to make sure our data is in its correct places 

locate_ffdates = function(data){
  uniq_dates = unique(total_score$date) 
  temp_var1 = NULL
  temp_var2 = NULL
  for (i in 1:length(uniq_dates)){   # Goes through all dates in total_score
    temp_var1   = data %>% dplyr::filter(date == uniq_dates[i])# filters out all the dates that are not in totalscore.
    temp_var2= bind_rows(temp_var1 ,temp_var2)
  }
  return(temp_var2) #Returns the data 
}

total_score$date = total_score$date %>% as.Date() # just making sure dates are in order as usual before i put it into the next func

#ff_data = ff_data %>% arrange(date) %>% filter(date <= total_score$date[1])
ff_dates = locate_ffdates(ff_data) #using the function on the FF data to get Risk free return and other data

index_dates = locate_ffdates(index)

selecter = c("date", "cum_ret") # Variables to have in top and bot tibble
index_dates2 = index_dates %>% ungroup() %>%  dplyr::select(all_of(selecter))
names(index_dates2)[2] = "index_cumret" 
  
total_score = merge(total_score, ff_dates, by.x="date", by.y="date") # Moving the dates into total_score df
total_score = merge(total_score, index_dates2, by.x="date", by.y="date")
```

## As previously mentioned the BOT performs very well. But this is due to the dotcom momentum bounce, then the 2008/2009 crash ! 

```{r graphing the end returns}
# Visualization of the scores 
colors <- c("top" = "blue", "bot" = "orange", "wml" = "green" , "index" = "red", "mid" = "brown")
# This top part is just to have a nice legend on the side of the graph 

total_score$date = total_score$date %>% as.Date()  # Make sure dates are DATES ! 

ggplot(total_score, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = mid, color = "mid"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       geom_line(aes(y = wml, color = "wml"), size = 1) + 
       geom_line(aes(y = index_cumret, color = "index"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Portfolio Cumulative Returns') +
    scale_y_continuous(breaks = seq(0,50,10)) +
    scale_x_date(date_breaks = '2 year',
               date_labels = '%Y',) +
    scale_color_manual(values = colors)

```

```{r cumalative returns for the ff data}
# Producing cumalative returns for the FF data
total_score$cu_ff_mkt_rf =  cumprod(1+total_score$ff_mkt_rf)
total_score$cu_ff_smb =  cumprod(1+total_score$ff_smb)
total_score$cu_ff_hml =  cumprod(1+total_score$ff_hml)
total_score$cu_ff_rf =  cumprod(1+total_score$ff_rf)
```

```{r plotting with some ff data}
# Just adding some more data to investigate 
# Do note that index and cumulated fama french market and risk free rate is very similar.
colors <- c("top" = "blue", "bot" = "orange", "wml" = "green" , " cu_ff_mkt_rf" = "red", "index" = "black")
total_score$date = total_score$date %>% as.Date()  

ggplot(total_score, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       #geom_line(aes(y = mid, color = "mid"), size = 1) + 
       geom_line(aes(y = wml, color = "wml"), size = 1) + 
       geom_line(aes(y = cu_ff_mkt_rf , color = " cu_ff_mkt_rf"), size = 1) + 
       geom_line(aes(y = index_cumret, color = "index"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Portfolio Cumulative Returns') +
    scale_y_continuous(breaks = seq(0,50,5)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```


```{r calculating returns for deciles and formating the tables}
#
# Before implementing a time series we need to split top and bot into monthly returns
#
month_ret = function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  tot_ret = c() # empty vector 
  sum_w = 0
  temp = 0 
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest 
      ret = (z$return)  # This is a bit redundant
      tot_ret[i]  = sum(ret) # Summarize all the returns in a month 
    }
return(tot_ret) #return the returns for each month out of the loop based o
}

month_ret_top = month_ret(top) %>% bind_cols(unique(top$date)) %>% as_tibble() # Binding uniqe dates 
names(month_ret_top)[1] = "ret_mont_top" # Rename cols 
names(month_ret_top)[2] = "date"

month_ret_bot = month_ret(bot) %>% bind_cols(unique(bot$date))
names(month_ret_bot)[1] = "ret_mont_bot"
month_ret_tot = cbind(month_ret_top,month_ret_bot$ret_mont_bot)
names(month_ret_tot)[3] = "ret_mont_bot"

month_ret_mid = month_ret(mid) %>% bind_cols(unique(mid$date))
names(month_ret_mid)[1] = "ret_mont_mid"
month_ret_tot = cbind(month_ret_tot,month_ret_mid$ret_mont_mid)
names(month_ret_tot)[4] = "ret_mont_mid"

month_ret_tot = month_ret_tot %>% arrange(date)

month_ret_tot = cbind(month_ret_tot,index_dates$return)
names(month_ret_tot)[5] = "index_ret"

month_ret_tot = month_ret_tot %>% relocate(date, .before = ret_mont_top) # OCD kicking in, moving date to front col 

month_ret_tot %>% head()
```

```{r making some time series for later}

ts_index_ret = ts(index_dates$return, start = c(start_year, 1), freq = 12) # Creating time series
ts_top_ret = ts(month_ret_tot$ret_mont_top , start = c(start_year, 1), freq = 12)
ts_mid_ret = ts(month_ret_tot$ret_mont_mid , start = c(start_year, 1), freq = 12)
ts_bot_ret = ts(month_ret_tot$ret_mont_bot , start = c(start_year, 1), freq = 12)
```

```{r adding betas for alpha calculation}
beta = cov(ts_top_ret, ts_index_ret)/var(ts_index_ret) # This is equal to the one above, to check the function! 
total_score$alpha = ts_top_ret - total_score$ff_rf - beta * (total_score$ff_mkt_rf - total_score$ff_rf) # calculating alpha.
```

#
# Here we make the rolling regression, But first we normalize it with the "normalize function" so we can get numbers between 0 and 1 
# This is becouse our dataset is very small, and we dont want to loose data. Additionaly we are investigating volatility so the numbers 
# We will use still has a ratio to find volatility

```{r normalizing returns, and 6m rolling regression to get Betas}
library(rollRegres) # Using rolling regression 

normalizefunc = function(portfolio, colnumb){ # Making the normf. funk 
  x = portfolio[colnumb]
  y = (x-min(x))/(max(x)-min(x))
  return(y)
}

month_ret_tot$norm_ret_mont_top = normalizefunc(month_ret_tot, 2)  # Normalizing 
month_ret_tot$norm_ret_mont_bot = normalizefunc(month_ret_tot, 3)
month_ret_tot$norm_ret_mont_mid = normalizefunc(month_ret_tot, 4)
month_ret_tot$norm_index_ret = normalizefunc(month_ret_tot, 5)

month_ret_tot$norm_ret_mont_top = as.numeric(as.character(unlist(month_ret_tot$norm_ret_mont_top))) # Unlisting etc becouse we need to 
month_ret_tot$norm_ret_mont_bot = as.numeric(as.character(unlist(month_ret_tot$norm_ret_mont_bot))) # Or else we have problems regressing
month_ret_tot$norm_ret_mont_mid = as.numeric(as.character(unlist(month_ret_tot$norm_ret_mont_mid))) 
month_ret_tot$norm_index_ret = as.numeric(as.character(unlist(month_ret_tot$norm_index_ret)))

month_ret_tot[sapply(month_ret_tot, is.infinite)] = 0.99999 # Dummy encoding since log cant be 0 or 1 we convert to dummy vars 
month_ret_tot[month_ret_tot == 0] <- 0.000001 # Dummy encoding
month_ret_tot[month_ret_tot == 1] <- 0.999999 # Dummy encoding

reg_top = roll_regres(norm_ret_mont_top ~ norm_index_ret, month_ret_tot, width = 6, #Beta top vs index, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))

reg_bot = roll_regres(norm_ret_mont_bot ~ norm_index_ret, month_ret_tot, width = 6, #Beta top vs index, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))

reg_mid = roll_regres(norm_ret_mont_mid ~ norm_index_ret, month_ret_tot, width = 6, #Beta top vs index, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))


reg_top_sum = bind_cols(month_ret_tot$date ,as.data.frame(reg_top$coefs)) # Binding it together  in a new df 
reg_bot_sum = bind_cols(month_ret_tot$date ,as.data.frame(reg_bot$coefs)) 
reg_mid_sum = bind_cols(month_ret_tot$date ,as.data.frame(reg_mid$coefs))

# names(month_ret_tot)[5] = "norm_ret_mont_top" # Renaming cols 
# names(month_ret_tot)[6] = "norm_ret_mont_bot"
names(month_ret_tot)[9] = "norm_ret_index"

names(reg_top_sum)[1] = "date" #renaming cols 
names(reg_top_sum)[3] = "beta_top"

names(reg_bot_sum)[1] = "date" # Guess what? We are renaming cols 
names(reg_bot_sum)[3] = "beta_bot"

names(reg_mid_sum)[1] = "date" # Guess what? We are renaming cols 
names(reg_mid_sum)[3] = "beta_mid"

reg_top_sum[sapply(reg_top_sum, is.na)] = 0 # Setting NAs to Zero
reg_mid_sum[sapply(reg_mid_sum, is.na)] = 0
reg_bot_sum[sapply(reg_bot_sum, is.na)] = 0

devtools::unload("rollRegres") # Unload the pack 
```

# Now we have found the Betas, we wish to highlight the TOP and BOT performance under a crash.
# From the graph below we can clearly see bot is providing significantly better resaults then top during the "bounce back" 
# after a momentum crash

```{r plotting log normalized beta}
# Choose from and too dates here to zoom in on graph
from_date = "2008-12-30" 
to_date = "2010-01-01"
##
##
reg_top_sum$date = reg_top_sum$date %>% as.Date() # Becouse sometimes it just wants to be reminded its a date.....

filtered_reg_top_sum = reg_top_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Using filter to zoom
filtered_reg_mid_sum = reg_mid_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date)
filtered_reg_bot_sum = reg_bot_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Filter to zoom 


colors <- c("top" = "blue", "bot" = "green", "mid" = "red") # Getting the nice legend on the side, and choose colours 
ggplot(filtered_reg_top_sum, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = beta_top, color = "top"), size = 0.5) +
       geom_line(data = filtered_reg_bot_sum, aes(y = beta_bot, color = "bot"), size = 0.5) +
       geom_line(data = filtered_reg_mid_sum, aes(y = beta_mid, color = "mid"), size = 0.5) +
       labs(x = 'Date',
       y = '6 month rolling beta ',
       title = 'Top, Mid and bot betas') +
    scale_y_continuous(breaks = seq(-1,1,0.5)) + # Range of messurements 
    scale_x_date(date_breaks = '1 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```

```{r plotting alpha}
ggplot(data=total_score,aes(x=date)) + geom_line(aes(y=alpha, color="alpha"),colour="#009682")+
  xlab("Date")+
  ylab("Alpha")
```

# This is the US reccession indicator. This is just here to see that it plots correctly, as we will use it in some graphs later on 

```{r bearmarket indicator}
# Recession indicators retrived from https://fred.stlouisfed.org/series/USREC

rec_ind = read.csv("USREC.csv") #load 

rec_ind$date= rec_ind$DATE %>% as.Date() # dates are dates 
rec_ind= dplyr::select(rec_ind, c(date, USREC)) # select what we need 

day(rec_ind$date)=days_in_month(rec_ind$date) #find last day in months, and set dates to our standard dates 

dates_rec_ind = locate_ffdates(rec_ind) #using the function from the FF data bind our dataset to the correct values 
  
total_score = merge(total_score, dates_rec_ind, by.x="date", by.y="date") # Merges this FF dataset with Maindataset 

total_score$bear_market = total_score$USREC*6.5 # Just multiplying the 1 so its easier to see on a graph 

ggplot(data=total_score,aes(x=date)) + # Plotting 
            geom_area(aes(y=USREC,color="red"),fill=rgb(red = 1, green = 0, blue = 0, alpha = 0.5))+
            geom_line(aes(y=top, color="top"))+
            xlab("Date")+
            ylab("Dollar value of investment")+
            ylim(0,6.5)

```

#
# We test for stationarity white noise etc 
#
```{r testing top for stationarity, white noise auto regression}
library(aTSA)
library(forecast)

# from calculating betas we have the ts from top bot and s&p

testing_ts = function(timeseries, lag) {
  plot(timeseries)
  plot(timeseries^2)
  acf(timeseries, lag.max=lag) #setting the lag, checking for autocorrelation
  adf.test(timeseries) # p-value > 0 .05 therefor we reject the hypothesis that it is nonstationary. Therefor it is stationary.
  pacf(timeseries)
  Box.test(timeseries, lag = lag, type = "Ljung") # White noise test 
  decompose_ts_top = decompose(ts_top_ret) # Decomposing the inputs 
  decompose_ts_top %>% plot() #plotting when done 
}

decompose_ts_top = diff(ts_top_ret, differences = 1)  # creating a first order decomposition of the time series
testing_ts(decompose_ts_top,6)
lambda = BoxCox.lambda(decompose_ts_top) # lambda, this is here from a previous code, (might be nice to have around )

```


```{r, volatility calculations}
# Calculating volatility for our portfolios over 6 month, with a N=12 (meaning months in our instance)

vol_calc = function (data){
  out = volatility(data, n = 6, calc = "close", N = 12, mean0 = FALSE)
  return(out)
}

month_ret_tot$top_vol = vol_calc(month_ret_tot$norm_ret_mont_top)
month_ret_tot$index_vol =  vol_calc(month_ret_tot$norm_ret_index)
month_ret_tot$bot_vol = vol_calc(month_ret_tot$norm_ret_mont_bot)

```

```{r, making time-series of index for testing}
# Some of the functions require us to use timeseries, therefor we convert them here 

temp_index_ret = month_ret_tot %>% dplyr::select("index_ret","date") %>% arrange(date)
temp_close_sp = index_dates  %>% ungroup() %>% dplyr::select("close","date") %>% arrange(date)
temp_index_rvol = month_ret_tot %>%  dplyr::select("index_vol","date") %>% arrange(date)

row.names(temp_index_rvol) = temp_index_rvol$date
row.names(temp_close_sp) = temp_close_sp$date
row.names(temp_index_ret) = temp_index_ret$date

# temp_close_sp = temp_close_sp %>% dplyr::select(-c("date"))
# temp_index_ret = temp_index %>% dplyr::select(-c("date")) # Used for bugfixing, removing the date column 
# temp_index_rvol = temp_index_rvol %>% dplyr::select(-c("date"))

ts_index_close = ts(temp_close_sp[,1], start = c(start_year, 10), freq = 12) # closing prices
ts_index_ret = ts(temp_index_ret[,1], start = c(start_year, 10), freq = 12) # returns  
ts_index_rvol = ts(temp_index_rvol[,1], start = c(start_year, 10), freq = 12) # real volatility
```


# Fitting Garch model
we fit this garch to investigate a fixed volatility strategy.
```{r}
library(rugarch)
# Importing our garch model to fit the data on, in this case we use the return of top portfolio 


temp_top_ret = month_ret_tot %>%  dplyr::select("ret_mont_top","date") %>% arrange(date) # Ordering 
ts_temp_top_ret = ts(temp_top_ret[,1], start = c(start_year, 10), freq = 12) # Making time series 


garchspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), # Setting the specs for our GJR-Garch model
                        variance.model = list(model = "gjrGARCH"), 
                        distribution.model = "sstd")


garchfit = rugarch::ugarchfit(data = ts_temp_top_ret  , spec = garchspec) # Estimating the model 

garchvol = sigma(garchfit)# Use the method sigma to retrieve the estimated volatilities 


plot(garchvol) # Plot the volatility

sqrt(uncvariance(garchfit)) # THis will be a basis for our manual tweaking later, as i was not able to implement the dynamic garch. And i worked 3 days on this.

devtools::unload("rugarch") # Dropping the pack
```

# 
# Fixed "dynamic" weights volatility 
We set set the dynamic weights to a set interval based on ploted garch vol from last chunk. 
Even though we are not able to circumvent the drop, we join bottom portfolio to the downside but we will also join its rebound
after the rebound since the volitility has dropped, we continue to invest in top portfolio.
 
```{r, cumalative dynamic portfolio}
#Here we we run for each month, if the volatility is higher then 2.22 we invest in bot portfolio and not top, then we take those returns 
#next month and we reinvest them in another portfolio (rebalancing )
cumalative_ret_dynamic = function (portfolio,portfolio2, gjr_vol){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  tot_ret = c() # empty vector 
  border = 2.2 # Selecting our volitility cap 
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      if (i == 1){  
        if (gjr_vol[i,1] >= border){ # High volatility invest in bot portfolio 
          x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date) # filters dates to get right portf 
          ret_x = (1+x$return) * x$vw # invests on the different stocks value weighted 
          tot_ret[i] = sum(ret_x) # sums it up so we can reinvest it next month 
        } 
        else {
          z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) 
          ret_z = (1+z$return) * z$vw # If garch vol is low invest in top portf 
          tot_ret[i] = sum(ret_z)
              }
                }
      if (i > 1){ 
          if (gjr_vol[i,1] >= border){ # we do the same as above, with minor tweaks 
            x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date)
            ret_x = tot_ret[i-1] * (1+x$return) * x$vw
            tot_ret[i] = sum(ret_x)
          }
          else{
            z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date)
            ret_z = tot_ret[i-1] * (1+z$return) * z$vw
            tot_ret[i] = sum(ret_z)
          }
        }
    }
  return(tot_ret) #returns the calculations out of the loop
  }

portstart_date

garchvol1 = garchvol %>% as.data.frame() # This is just fitting the garchvol to our dataset 
garchvol1 = cbind(garchvol1, total_score) 
colnames(garchvol1)[1] = "gjr_vol" 
rownames(garchvol1)=NULL
garchvol1 = garchvol1 %>% arrange(date) %>% dplyr::select(c("gjr_vol","date"))
garchvol1 = garchvol1 %>% dplyr::filter(date > portstart_date)

dynamic = cumalative_ret_dynamic(top,bot,garchvol1) # runs the script


dynamic %>% plot(type = "l") # fast plot to see outcome 

arrange(total_score,date) # Just making sure dates are in order before we move dynamic resaults into date 
tot_score_final = cbind(total_score, dynamic) # binding it to our main table 
```


#Here we plot the main findings. 
Here we can clearly see our "dynamic" is doing very well. HOWEVER, after investigating the data, we can see this performance is due the cumulative returns. This can partly be explained by the 2000s dotcom bubble, dynamic gains extreme momentum at the start, as we would be in bot port then we switch to top (changing starting dates to 2005 yields a very different graph!). We also have a bias between the monitor and keyboard, as I tuned the "border" variable to give us a nicer graph. 

Another finding is how "hard" the "dynamic" droped in 2009 as it would have higher volatility triggering the allocation to bot portfolio, therefor it most likely has the same shape as bot in the same timeframe. 

```{r}
colors = c("top" = "blue", "bot" = "orange", "set_dynamic" = "green", "index" = "red", "USREC" = "purple")

ggplot(tot_score_final, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       geom_line(aes(y = dynamic, color = "set_dynamic"), size = 1) + 
       geom_line(aes(y = index_cumret, color = "index"), size = 1)+
        geom_line(aes(y = USREC/3, color = "USREC"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Fixed "Dynamic" weights @ 2.3 vol') +
    scale_y_continuous(breaks = seq(0,10,1)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```

```{r}
library(rugarch)
library(fGarch)

x = cbind(month_ret_tot$date,month_ret_tot$ret_mont_top) %>% as.data.frame()  #Binding dates to return top 
x$V1 = x$V1 %>% as.Date() # Formating dates 
x = x %>% arrange(V1) # Sort after dates 
y = x$V2  # Y = V2 (our data)


x$norm_sp = (y-min(y))/(max(y)-min(y)) # transforms data to be within 0 and 1 for log  

row.names(x) = x$V1 # Set rownames to dates 

ts_sp_test = ts(x[3], start = c(start_year,10), frequency = 12 ) # Making the index timeseries 

garchspec <- ugarchspec(mean.model = list(armaOrder = c(0, 0), archm = TRUE, archpow = 2), variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), distribution.model = "sstd") # Ugarch model specifications 


garchfit <- ugarchfit(data = ts_sp_test, spec = garchspec) # Fitting the model on our SP data 

plot(fitted(garchfit)) # Plot our fitted model 
```

```{r}
index_tables = total_score # Renames our df to avoid confusion later on 

ts_sp = ts(month_ret_tot$norm_ret_index, start = c(start_year,10), frequency = 12 ) # Time series creation of index 

garchspec = ugarchspec(mean.model = list(armaOrder = c(0, 0), archm = TRUE, archpow = 2), # Specifying the model
                        variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), 
                        distribution.model = "sstd")


garchfit = ugarchfit(data = ts_sp, spec = garchspec)  # fitting timeseries index to the garch model 

coefs = coef(garchfit) # Extract our garch coefs. for later 

plot(fitted(garchfit)) # fast plot 
```


```{r, volatility calculations}

vol_calc = function (data,period){
  width = period
  nulls = rep(0, width-1) %>% as_tibble() # Fills our non calculatable rows with zero (first 6)
  temp = rollapply(data, width = width, FUN = "sd") %>% as.tibble() # Function to roll 
  temp = temp * sqrt(temp) 
  out = rbind(nulls,temp)
  return(out)
}

width = 6

x = vol_calc(month_ret_tot$norm_ret_mont_top,width) # Rolling windows of volatility on top 
index_tables$top_vol_6 = x$value

x = vol_calc(month_ret_tot$norm_ret_index,width) # Same but index 
index_tables$index_vol_6 = x$value

x = vol_calc(month_ret_tot$norm_ret_mont_bot,width) #Same but for bot
index_tables$bot_vol_6 = x$value
```

```{r}
forward_vol_calc = function (data){
  out1 = NULL
  out2 = NULL
  for (i in 1:length(data)){
    out1 = ((data[1+i] - data[i]) * sqrt(2))
    out2= c(out2,out1)
  }
  return(out2)
}

index_tables$f_top_vol = forward_vol_calc(month_ret_tot$norm_ret_mont_top) #Forward volatility calculations in accordance with the paper. 
```

```{r}
# Regressing our top momentum portfolio

reg_ftop_sigma = lm(index_tables$f_top_vol ~ garchfit@fit$sigma) # On sigma 
reg_ftop_126 = lm(index_tables$f_top_vol ~ index_tables$top_vol_6) # on vol_6

summary(reg_ftop_sigma) #Shows resaults 
summary(reg_ftop_126)
```


```{r}
# We need to make rolling variance first
ts_index_v1 = index_dates %>% dplyr::select(return,date) #select 
ts_index_v1 = ts_index_v1 %>% ungroup() %>% dplyr::select(-c(symbol)) # Kickout what we dont need

a = c(0,0,0,0,0) %>% as.matrix # 0's to fill in for later 
rollinvar = rollapply(index_dates$return, width = 6, by = 1, FUN = "var") %>% as.tibble() #rolls var

colnames(a)[1] = "value" # new colname 
matchroll = rbind(a,rollinvar) # binds the zeroes to our vars, to get equal length of df 

y_hat0 = 1.973/100 # Vars from paper 
y_int = -0.397 # Vars from paper 

index_tables = index_tables %>% mutate(yhat = y_hat0) # Create new cols with the vars 
index_tables = index_tables %>% mutate(yint = y_int)
index_tables = index_tables %>% mutate(roll_6_var = matchroll$value)

index_tables$roll_6_var %>% plot(type = "l") #plotting to investigate 
index_tables$f_top_vol %>% plot(type = "l") #

# Bear indicator = USREC 
index_tables[28:ncol(index_tables)] # just to view the tail of our dataset

errorterm = 0 # Expected value of noise

#formula below extracted from book 
index_tables$r_hat = index_tables$yhat + (index_tables$yint * index_tables$USREC * index_tables$index_vol_6 ) + errorterm

```

```{r}
#
# Come back to this, we need daily to retrive variance for 22 days and 126 days 
#
  
index_tables$garch_res = garchfit@fit$residuals # Finding residuals 
  
  
index_tables$binary_error[index_tables$garch_res > 0] = 0 # Following the procedure from paper, above 0 = 0
index_tables$binary_error[sapply(index_tables$binary_error, is.na)] = 1 # Nas will technicaly 0 and are therefor set to 1 
  
omega = coefs[3] # Just putting the extracted coefs into variable names 
beta = coefs[5]
alpha = coefs[4]
gamma = coefs[6]


# formula from paper, some parts removed due to multiplication by zero   
sigma_garch = omega + (beta*index_tables$top ) + (alpha + gamma * index_tables$binary_error) * index_tables$garch_res^2 

  
# sigma 22 is forward looking sigma, from book 
sigma22 = 0.001 + sigma_garch * 0.6114 + index_tables$top_vol_6 * 0.264

sig22mean = sigma22%>% mean() 
```


```{r}
v = month_ret_tot$ret_mont_top %>% mean() # mean 

scalar = (v / sig22mean) #scalar 

weights = (1/(2*scalar)) * (index_tables$r_hat/sigma22) *100

weights %>% plot(type = "l") # This is our weights 

# Do note how we have changed them in later chunks to avoid overleveraging 
```

```{r cumalative returns for each portfolio calc, warning = FALSE}
mod_weights = weights %>% as_tibble()
mod_weights$mod = weights %>% as.vector()

#################### This part i still here for those who want to experiment with and without leverage effects 
#################### Set modweights according to wishes.

mod_weights$mod[mod_weights$value > 1] = 1
 
#test = which(mod_weights$value < 1)
# for (i in 1:length(test)){
#     x = test[i]
#     mod_weights$mod[x] = weights[x]
# }
# 
# mod_weights$mod[mod_weights$value < -0.5] = -0.5

cumalative_ret_weights_portfolio = function (portfolio){ # Defining function
reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
tot_ret = c() # empty vector 
leveraged_ret = NULL
temp = NULL 
sum_return = NULL
diff = NULL
ret = NULL
z = NULL 
  for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
    if (i == 1){  # First round do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest 
      ret = (1+z$return) * z$vw #* mod_weights$mod   # Investing 1 dollar divided by stocks multiplied with returns
      sum_return = (sum(ret) / 1) - 1 
      leveraged_ret[i] = sum_return * mod_weights$mod[i]
      tot_ret[i]  = 1 * (leveraged_ret[i]+1) # Summarize all the returns from investing one dollar 
    }
    if (i > 1){ # When its beyond start date, (t+1). Do the following
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Filter out the dates not needed
      ret = (tot_ret[i-1] * z$vw * (1+z$return)) #* mod_weights$mod     #Returns * the weights 
      sum_return = sum(ret)
      diff = (sum_return / tot_ret[i-1])-1
      leveraged_ret[i] = diff * mod_weights$mod[i]
      tot_ret[i]  = tot_ret[i-1] * (1+leveraged_ret[i]) # Sum it all up to reinvest 
    }
  }
return(c(tot_ret,leveraged_ret)) #returns the calculations out of the loop
}

##
## This section retrives a string of numbers (return per rebalancing period)
##


top_score_dyn = cumalative_ret_weights_portfolio(top) # runs the script on top portfolio 
top_score_dyn = top_score_dyn %>% unlist() #unlists the results (technicality)

end = top_score_dyn %>% length() #Finds the end of our portfolio as we extracted multiple dates from the function
split = end / 2 # Splits sample into two 

returns = 1+top_score_dyn[1+split:end] %>% na.omit() # removes 1 na col at highest date, due to erorr calc

top_score_dyn = top_score_dyn[1:split] %>% na.omit() # Splits the data omits what is not needed 
```

```{r}
library(rollRegres) # Using rolling regression 

normalizefunc = function(portfolio){ # Making the norm. func 
  x = portfolio
  y = (x-min(x))/(max(x)-min(x))
  return(y)
}

top_score_dyn$norm_score = normalizefunc(returns)

top_score_dyn$norm_score[top_score_dyn$norm_score == 0] <- 0.000001 # Dummy encoding
top_score_dyn$norm_score[top_score_dyn$norm_score == 1] <- 0.999999 # Dummy encoding

reg_dyn = roll_regres(top_score_dyn$norm_score ~ month_ret_tot$norm_ret_index , width = 6, #Beta top vs index, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))

#Same as previous, normalizing incase we need it later.
```


```{r}
top_score_dyn = top_score_dyn %>% unlist()
tot_score_final = cbind(tot_score_final, top_score_dyn[1:(split)])
colnames(tot_score_final)[ncol(tot_score_final)] = "dynamic_garch" 
```

```{r}
colors = c("top" = "blue", "bot" = "orange", "set_dynamic" = "green", "index" = "red", "USREC" = "purple", "dynamic_garch" = "black")

ggplot(tot_score_final, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       geom_line(aes(y = dynamic, color = "set_dynamic"), size = 1) + 
       geom_line(aes(y = index_cumret, color = "index"), size = 1)+
       geom_line(aes(y = USREC/3, color = "USREC"), size = 1)+
       geom_line(aes(y = dynamic_garch, color = "dynamic_garch"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Monthly dynamic leverages only negative direction') +
    scale_y_continuous(breaks = seq(0,200,10)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)

```




```{r plotting log normalized beta}
# Choose from and too dates here to zoom in on graph
from_date = "2008-12-30" 
to_date = "2010-01-01"
##
##
reg_top_sum$date = reg_top_sum$date %>% as.Date() # Becouse sometimes it just wants to be reminded its a date.....

filtered_reg_top_sum = reg_top_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Using filter to zoom
filtered_reg_mid_sum = reg_mid_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date)
filtered_reg_bot_sum = reg_bot_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Filter to zoom 

new = cbind(reg_bot_sum$date, reg_dyn$coefs[,2]) %>% as_tibble()
new$V1 = new$V1 %>% as.Date()
colnames(new)[1] = "date"
new[sapply(new, is.na)] = 0 

filtered_reg_dynamic_sum = new %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date)


colors <- c("top" = "blue", "bot" = "green", "mid" = "red", "dynamic" = "black") # Getting the nice legend on the side, and choose colours 
ggplot(filtered_reg_top_sum, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = beta_top, color = "top"), size = 0.5) +
       geom_line(data = filtered_reg_bot_sum, aes(y = beta_bot, color = "bot"), size = 0.5) +
       geom_line(data = filtered_reg_mid_sum, aes(y = beta_mid, color = "mid"), size = 0.5) +
       geom_line(data = filtered_reg_dynamic_sum, aes(y = V2, color = "dynamic"), size = 0.5) +
       labs(x = 'Date',
       y = '6 month rolling beta ',
       title = 'Rolling betas') +
    scale_y_continuous(breaks = seq(-5,5,1)) + # Range of messurements 
    scale_x_date(date_breaks = '1 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```


## Table making 
```{r}
library(gt)
ts_ff_rf = ts(total_score$ff_rf, start = c(start_year, 1), freq = 12)

table_formating = function(data,benchmark) {
  temp = NULL
  temp$beta = cov(data, benchmark)/var(benchmark)
  temp$alpha = CAPM.alpha(data,benchmark, Rf = mean(ts_ff_rf)) # Setting RF rate to mean of FF Risk free rate
  temp$sharp = SharpeRatio(data, Rf = 0, p = 0.95, FUN = "StdDev", weights = NULL, annualize = FALSE)
  temp$skew = skewness(data, na.rm = FALSE, method = "fisher" )
  temp$stddev = StdDev(data)
  #temp$ret_riskf = data - ts_ff_rf
  return(temp)
}

char_top = table_formating(ts_top_ret,ts_index_ret) %>% as_tibble()
char_mid = table_formating(ts_mid_ret,ts_index_ret)%>% as_tibble()
char_bot = table_formating(ts_bot_ret,ts_index_ret)%>% as_tibble()

char_tot = rbind(char_top,char_mid,char_bot)
char_tot = char_tot %>% mutate (name = c("top","mid","bot"))
char_tot = char_tot %>% relocate(name, .before = "beta")
char_tot %>% gt()

```