---
title: "Data retrival and random selection of 100 sample stocks"
output: nils
---

```{r}
library(tidyquant)
library(tidyverse)
library(dplyr)
library(plyr)
library(plyr)
```

# ```{r}
# library(devtools)
# devtools::install_github("business-science/tidyquant")
# ```


```{r}
##
# This sections registers the do parallel for your computer.
##

   library(doParallel)
   library(snow)
   library(doSNOW)
 
<<<<<<< HEAD
cl <- makeCluster(detectCores()-2, type = "SOCK") #Finds max hypertread on your pc minus two (for stability) 
=======
cl <- makeCluster(detectCores()-2, type = "SOCK")
>>>>>>> 25af62a471876fd5577f36454ead99964600f12c
 
call_cluster_init = function(cl){ # Creates a funtion to call under parallalization  
   registerDoParallel(cl)
 }
```

```{r}

nyse = tq_exchange("NYSE") # Retrives all tickers from NYSE
nasdaq = tq_exchange("NASDAQ")# Same but Nasdaq  
amex = tq_exchange("AMEX") # Amex   

all_ticks = rbind(nyse,nasdaq,amex) # Binds all stockmarkets together  
all_ticks = all_ticks$symbol # MAkes a variable where all tickers are found 
```

```{r}
##
# THIS TAKES A LONG TIME, To save time adjust dates as it retrives all stocks available and omits stocks without historical data 
##


total_loop_time = proc.time() #Starts timer 
tot_df = tq_get(all_ticks,
                     from = '1960-01-01',
                     to = '2021-10-24',
                     get = "stock.prices")
#save(tot_df, file="tot_df.Rdata") # Untick this box to save data to your disk  
#write.csv(tot_df,"/home/nnx/Machinelearning 1/Momentum-Crashes/tot_df_db.csv", row.names = FALSE) #Write it as csv to your disk. This file is 3-5 times bigger then .Rdata but can be used in databases, excel etc. 
proc.time() - total_loop_time #stops timer Just to see how long it uses 
```


```{r}
<<<<<<< HEAD
#
# To avoid selection bias! we select numb_stock = n, at random. 
#
=======
total_loop_time = proc.time()
filter_data = NULL

randomizer = function(x){
  filter_data = tot_df %>% filter(symbol == temp_stock[random_stocks[x]])
  return(filter_data)
}

random_stocks = lapply(1:numb_stocks, randomizer)

df = ldply(random_stocks)
df = subset(df, select = -c(value)) #removing a redundant column

save(df, file="df.Rdata")
proc.time() - total_loop_time
```

```{r}
total_loop_time = proc.time()
filter_data = NULL

call_cluster_init()
>>>>>>> 25af62a471876fd5577f36454ead99964600f12c

numb_stocks = 5000  # Set to lower number to get less random drawn stocks.

<<<<<<< HEAD
temp_stock = unique(tot_df$symbol) # Find all the uniqe ticker from previously downloded data
random_stocks = sample(1:length(temp_stock), numb_stocks, replace= FALSE) # Creates a random sample of numb_stocks from the entire stock universe. 

=======
para_loop = foreach (i = 1:numb_stocks, .packages=packs_needed) %dopar% {
                      filter_data = tot_df %>% filter(symbol == temp_stock[random_stocks[i]])
                      return(filter_data)
}

df = ldply(para_loop)
df = subset(df, select = -c(value)) #removing a redundant column

save(df, file="df.Rdata")
stopCluster(cl)
proc.time() - total_loop_time
>>>>>>> 25af62a471876fd5577f36454ead99964600f12c
```


# ```{r}
# total_loop_time = proc.time()
# filter_data = NULL
# 
# randomizer = function(x){
#   filter_data = tot_df %>% filter(symbol == temp_stock[random_stocks[x]])
#   return(filter_data)
# }
# 
# random_stocks = lapply(1:numb_stocks, randomizer)
# 
# df = ldply(random_stocks)
# df = subset(df, select = -c(value)) #removing a redundant column
# 
# save(df, file="df.Rdata")
# proc.time() - total_loop_time
# ```

```{r}

#####################################
########
######## DO NOT RUN! UNLESS PC HAS OVER 45GB OF RAM @ 30 cores/parallels      
########  USE CHUNK ABOVE IF ON REGULAR COMPUTER!
########     CAN OVERLOAD PC AT MAX CORES
########
#####################################
total_loop_time = proc.time() # Timer 
filter_data = NULL # Empty variable

call_cluster_init() # Calling the parallel clustering

packs_needed = c("tidyverse", "dplyr") # Need to call packs inside parallelization, or else they do not exist within the loop- 

para_loop = foreach (i = 1:numb_stocks, .packages=packs_needed) %dopar% { # makes a loop  
                      filter_data = tot_df %>% filter(symbol == temp_stock[random_stocks[i]]) # filter out all stocks that are not "picked" by the random generator 
                      return(filter_data) # returns data
}

df = ldply(para_loop) # This is here to convert from x amount data from para_loop into single dataset.
df = subset(df, select = -c(value)) #removing a redundant column

save(df, file="df.Rdata") #Saving the data to the pc 
stopCluster(cl) # Closing down the computing cluster 
proc.time() - total_loop_time # stops the timer 

# Freeing up ram as these are not needed anymore
para_loop = NULL
nyse = NULL
nasdaq = NULL
amex = NULL
```

####
## Testing set for program flow. 
####
# #tickers = c('BND', 'VB', 'VEA', 'VOO', 'VWO', 'aapl', 'msft')
# 
# #Old stocks pre year 1911
# tickers = c('xom', 'ge', 'cvx', 'ibm', 'cl', 'pg', 'jnj', 'jpm','pfe', 'ko', 'wfc', 'c', 'bac', 'mrk', #'pep', 'abt', 'gs', 'ups', 'mmm', 'axp', 'f', 'cvs', 'usb', 'unp', 'bmy', 'dd', 'met', 'lly', 'dow', 'EMR')


