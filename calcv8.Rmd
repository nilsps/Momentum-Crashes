---
title: "Momentum Crashes"
output:
  pdf_document: default
  'html_document:': defaultv
  html_document: default
editor_options:
  chunk_output_type: inline
---

#
# NOTE TO READER 
# 
# This dataset has survivorship biases in it! As we we dont take into considiration when stocks are leaving or not
# Additionaly due to the date (2000+) bot performs very well due to the bounce back from the momentum crash of the dotcom bubble! 
# I chose to hold it there so we could see it

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, packloading}
library(dplyr)
library(tidyquant)
library(tidyverse)
library(plyr)
```

```{r, read data}
library(readr)

df = read_csv2("sp500.csv",col_names = TRUE, col_types = cols(`date` = col_character(), #Loading the data 
        `mcap` = col_number(), `close` = col_number(), 
        `tot_ret` = col_number()))

names(df)[2]  = "symbol" # Change the name of col 2 

df[is.na(df)] = 0 # Set NAs to zero 

devtools::unload("readr") #unload the pack
```

```{r, prepare data}
library(bsts)
##
## Cleaning some data, removing some dates
##
df$date =  as.Date(df$date, format = '%d.%m.%Y') # Setting the date to standard format 

portstart_date = c("2000-01-01") # Portfolio start from the dataset

df = dplyr::filter(df, date > portstart_date) # Filter the dates 

df = df %>% dplyr::filter(close != 0) %>% dplyr::filter(mcap != 0) %>% dplyr::select(-(tot_ret)) #MCAP and close price cannot be zero 

transform(df, date = bsts::LastDayInMonth(df$date)) # Changeing the dates to the last day in the month

df = df %>% group_by(symbol) %>% arrange(date)  # Group and arrange 

devtools::unload("bsts") # unload pack 
```

```{r, return calc pers stock}
# Return calculation for each stock in. More accurate then the "simple" lead - var /var method 
# Normal methods gives high random scores, as it tries to calculate with next col
# This method returns NA when there is no other date to calculate to (no next lead)

return_calculations = function(portfolio){
  reb_month =  unique(portfolio$date) # Find all unique stocks 
  reb_stock = unique(portfolio$symbol) # Find all unique dates 
  temp_holder = NULL # working variable 
  temp_holder2 = NULL # Working variable 
  pb = txtProgressBar(min = 0, max = length(reb_stock), initial = 0, style = 3) # Progressbar 
  count = 1 # Counter to see time remaining
    for (j in 1:length(reb_stock)){ # For each iteration of all stocks  do this 
      temp_holder = portfolio %>% dplyr::filter(symbol == reb_stock[j]) # Filter to calculate pr stock 
      temp_holder$return  = ((lead(temp_holder$close) - temp_holder$close)  / temp_holder$close) # Return calculations
      temp_holder2 = rbind(temp_holder,temp_holder2)  # Bind it together with a empty var then add more when it comes 
      count = count + 1 # increase the counter for the function
      vv = setTxtProgressBar(pb,count) # set the progressbar to update 
    }
  return(temp_holder2) # Return the entire table with returns 
}  

df = return_calculations(df) # Runs function
df = df %>% dplyr::filter(date < "2021-10-01") # Last calculations does not have anything above to calculate return from, therefor rem. 
```




```{r, 2:12 month return to messure momentum}
# This creates 2:12 cols, these cols will tell you if you sold an asset today, how much would you gain since you bought it 

return_function = function(portfolio,period){ # Defining function to be run in func
  x = ((portfolio$close - lag(portfolio$close, n = period))  / lag(portfolio$close, n = period)) # return with iterations of lag
  return(x) # return
}

mom_2_12= function (portfolio){ # Defining function
  uniq_stocks =  unique(portfolio$symbol) # finds each stock to calc returns
  temp_holder = 0 %>% as.data.frame()
  temp_holder2 = 0 %>% as.data.frame() # These are just working variables 
  temp_holder3 = 0 %>% as.data.frame()
  count = 0
  pb = txtProgressBar(min = 0, max = length(uniq_stocks), initial = 0, style = 3) # progress bar
   z = 0 %>% as.data.frame() # predefine a new datatable to work with 
  for (i in 1:length(uniq_stocks)){ # for each date we have in the portfolio do this
      z = portfolio %>% dplyr::filter(uniq_stocks[i] == symbol) # filter each stock 
      count = count + 1 # Adds 1 to the counter for progressbar 
       setTxtProgressBar(pb,count) # Starting the progress bar
      for(j in 2:12) { # for each stock do this 12 times (1 is skipped per the paper)
        #temp_holder  = momentum(z$return, n=j)
        temp_holder = return_function(z, j) # Call function from above 
        name = paste0("mom", sep = "_", j) # Creates the name mom and pasts j to it, so it becomes mom_2 mom_3 etc
        z[[name]] =  temp_holder # Names the column
        if (j == 12){ 
          temp_holder2  = bind_rows(temp_holder2, z) # at the end of the "run" bind it together for later export
        } 
     }
    }
  return(temp_holder2) # returns the entire updated table 
}  

test  = mom_2_12(df) # Test is just temp name, but this calls the functions from above 
df = test %>% slice(-c(1)) # Due to bind with one Zero, there is a redundant row, this is removed here. 
test = NULL # Put test df to null
```

```{r, scrubbing data, making som return rankings}
# We can get data that is calculated to inf! this is due to the table sometimes provides us with nothing to calculate to or from \ incase a delesting etc. Therefor we have to convert info to get Zero as a value, if we filter out inf we will remove to many returns! 

library(matrixStats)

table_scrubber = function() { # Making a table scrubber function 
  df[7:17] = na_if(df[7:17], 0) # Setting NAs in mom_X columns to zero 
  df[sapply(df, is.infinite)] = NA # Infinate to NA 
  df[sapply(df, is.nan)] = NA # NANs to NA 
} 

#This is done to get clear tables, to use the functions below as we will count real values

table_scrubber() #Runs the function 
df$available_returns = rowSums(!is.na(df[7:17])) # compute number of available returns from mom_x (The rankings )

df$cum_return = matrixStats::rowProds(1+as.matrix(df[7:17]), na.rm=T) %>% log() #Creating cumalative returns as logs 

table_scrubber() # Just makeing sure my data is nice and shiny 

devtools::unload("matrixStats") # Unload old pack not needed anymore
```

```{r, actual ranking, over 8 month returns, normalizing values for quantiles}
# Before we input it into quantiles we have to normalize our data, as the function can only recive between 0 and 1. 
# As our data set goes from -1 to 1 

df$cum_return_norm = df$cum_return / df$available_returns #Normalising our returns devided on number on total number of rets
df = df %>% dplyr::filter(available_returns >= 8)# Above 8 monthly returns in ranking is put into the dataset
x = df$cum_return_norm  # Just renaming it to make it easier 
df$norm_norm_cret = (x-min(x))/(max(x)-min(x)) # Now we "normalize" the pre "normalized" returns to pipe it into the next function

x = NULL # Set x to null as its not needed. Sometimes I make diffrent vars for testing purposes, incase i need to check something.
```

```{r, putting into monthly deciles}
# Putting the stocks into deciles, ie top performers are in bracket 10 and bot in 1 

decile_seperation = function(portfolio){ #Create funk 
  x = portfolio #%>% dplyr::distinct(norm_norm_cret, .keep_all = T) # Redundant code, as it has problems if there are equal numbers to devide
  reb_month =  unique(x$date) # Finding all unique dates 
  temp_holder1 = NULL
  temp_holder2 = NULL # Empty working var 
  temp_holder3 = NULL # Empty working var 
  temp_holder4 = NULL # Empty working var
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
  count = 1 #counter for usage in progress bar
    for (j in 1:length(reb_month)){ # Takes all months with the mentioned criteria above.
      temp_holder1 = x %>% dplyr::filter(date == reb_month[j]) # filter each dates so we have one specific month
      temp_holder2$quantile = cut(temp_holder1$norm_norm_cret, # put it into dociles, based on monthly return
          quantile((temp_holder1$norm_norm_cret),
                    probs=seq(from=0,to=1,by=1/10), #Here you can change the deciles into quantiles etc if you wish 
                    na.rm =T),  # Defining deciles 
                    include.lowest=TRUE, 
                    labels=FALSE)
       temp_holder3= add_column(temp_holder1 ,as_tibble(temp_holder2)) # adds the decile score to the table 
       temp_holder4= bind_rows(temp_holder3 ,temp_holder4)  # Binds the score and the folder for a stock and date
       count = count + 1 # Adds 1 to the counter for progressbar 
       setTxtProgressBar(pb,count) # Starting the progress bar
    }  
  return(temp_holder4) # Returns from function a entire dataset with quantiles attached.
}

df = as_tibble(decile_seperation(df)) # calls the function on tibble df, and saves as dfa  

names(df)[names(df) == "value"] <- "quantile" # Renames the colume value to quantile.  

```

```{r, Monthly value weighted function}
# Creates monthly value weights per stock 

v_weights= function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  temp_holder = 0 %>% as.data.frame() # Working variable 
  z = 0 # Working var 
  count = 0 # setting the counter 
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == date) # Select the first date on the list, filter out the rest
      count = count + 1 # Adds 1 to the counter for progressbar 
      setTxtProgressBar(pb,count) # Starting the progress bar
        for (j in 1:10){ # In every single month from above, do the following in every decile 
          x = z # Working var 
          a = x %>% dplyr::filter(quantile == j) #Filter out each quantile 
          tot_w = sum(a$mcap)  # Sum all the market caps for that decile in a specific dates 
          a$vw  = (a$mcap/tot_w) %>% as.data.frame() # Create individual stock weighting based on market cap 
          temp_holder = bind_rows(a,temp_holder)  # Bind rows to get out put 
          if (j == 1 && i == 1) { # This is only here to remoe the one binding col!
             temp_holder = temp_holder %>% slice(-c(nrow(temp_holder)))
             } 
        }
      
    }
  return(temp_holder)
}  
        
df  = v_weights(df) # Calls the funk updates table 
df = df %>% slice(-c(ncol(df))) # slice of a redundant column 

#This is just to check the that the function sums to one "deeper" into the dataset
dfx = df %>% dplyr::filter(quantile == 1) %>% dplyr::filter(date == "2008-12-31") # Checking that weights sum to one 
sum(dfx$vw) # OK if it sums to one
dfx = NULL # Clears dataframe
```

```{r, Making bot and top quabtiles}
# Creating top deciles in own dataframes 
top_1 = df %>% dplyr::filter(quantile == 10) %>% arrange(date) # making the best performance portf
mid_1 = df %>% dplyr::filter(quantile == 5) %>% arrange(date)  # med performance portfolio based 
bot_1 = df %>% dplyr::filter(quantile == 1) %>% arrange(date)  # worst performance portfolio based 
```

```{r, retriving market index for sp500, cleaning and formating, THIS NEEDS TUNING IF CHANGE OF DATES!}
# We need the SP500, this is downloaded from the internett, then processed to match our data. 
## CREATING A MONTHLY DATASET for SP500!
##
library(bsts)
library(openair)

from = today() - years(40) #Takes from "today" and retrives 40 years of history
 
sp500 = tq_get(c("^GSPC"), get = "stock.prices", from = from) %>% #Get index prices (GSPC is SP500)
    group_by(symbol)


sp500$date =  as.Date(sp500$date, format = '%Y-%m-%d') # Changeing date format 

sp500 = dplyr::filter(sp500, date > portstart_date) # Filters out the dates to match our main dataset 

month_day = LastDayInMonth(sp500$date) %>% unique() %>% as.Date() # Retrives all last day in month, as we need the monthly data 

sp500 = sp500 %>% group_by(symbol) # group by symbol 

selecter = c("symbol", "date", "close") # Variables to have in top and bot tibble
sp500 = dplyr::select(sp500,all_of(selecter)) # Select the cols we want to work with 

sp500[1] = "SP500" # Rename all colls in the sp500 to sp500 for uniqeness 

#portfolio = sp500 # Dead test var, have it for bug fixing  

date_filter = function(portfolio) { # Filter function creation. This is to create final day of the month 
  counter = 0
  start_date = portfolio[1,2] # The start date is the first in the dataset
  start_date = start_date$date %>% as.Date(format = '%Y-%m-%d') # ISO standard date 
  end_date = portfolio[nrow(portfolio),2] # defines the end date 
  end_date = end_date$date %>% as.Date(format = '%Y-%m-%d') # Standarize 
  years = seq(from = start_date, to = end_date, by = 'year') # figure out how many years it is between the dates 
  years = length(years) # Finds length of years 
  temp_holder1 = NULL
  temp_holder2 = NULL
  temp_holder3 = NULL
  pb = txtProgressBar(min = 0, max = years, initial = 0, style = 3) # progress bar
  for (i in 1:years) { # For every year in the sp500 do this 
    year_counter = (2000 + counter) ###############HARDCODED 2000 NEEDS TO BE CHANGED ACCORDING TO YEAR! ###########
    x = selectByDate(portfolio, year = year_counter ) # Select each year 
    counter = counter + 1
    setTxtProgressBar(pb,counter) # Starting the progress bar
    for (j in 1:12){
      temp_holder1 = x$date[lubridate::month(x$date) == j]  # Finds when a month is the month of J 
      temp_holder1 = temp_holder1[length(temp_holder1)] # Finds the last day in the month 
      temp_holder3 = x %>% dplyr::filter(x$date == temp_holder1)# Filters out the rest of the days
      temp_holder2 = rbind(temp_holder2,temp_holder3) # Binding them together for output 
    }
      
  }
  return(temp_holder2) # Output
}

sp500 = date_filter(sp500) # Filters 

sp500 = sp500 %>% arrange(date) # arrange dates again 

sp500$return = ((lead(sp500$close) - sp500$close)  / sp500$close) # New col with returns 

sp500[is.na(sp500)] = 0 # Scrubbing 
sp500[sapply(sp500, is.infinite)] = 0 #Scrubbing 

sp500$cum_ret = cumprod(1 + sp500$return) # Create cumalative returns based on the SP500 

sp500 = sp500 %>% dplyr::filter(cum_ret != 0) %>% dplyr::filter(cum_ret != 1) # cannot have 0 or 1 in cumalative return 
# This is due to how we calculate returns, as first month does not have returns to calculate from 

devtools::unload("bsts") #pack unload
```

```{r, cumalative returns for each portfolio calc, warning = FALSE}
###
### Calculates returns on specific portfolios, invest 1 dollar in time Zero and does compounding Value WEIGHTED
###### 
selecter = c("symbol", "date", "return", "vw") # Variables to have in top and bot tibble
top = dplyr::select(top_1,all_of(selecter)) # Selects from selecter
mid = dplyr::select(mid_1,all_of(selecter)) # Selects from selecter
bot = dplyr::select(bot_1,all_of(selecter)) # Selects from selecter


cumalative_ret_portfolio = function (portfolio){ # Defining function
reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
tot_ret = c() # empty vector 
sum_w = 0
temp = 0 
  for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
    if (i == 1){  # First round do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest 
      ret = (1+z$return) * z$vw  # Investing 1 dollar divided by stocks multiplied with returns
      tot_ret[i]  = sum(ret) # Summarize all the returns from investing one dollar 
    }
    if (i > 1){ # When its beyond start date, (t+1). Do the following
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Filter out the dates not needed
      ret = tot_ret[i-1] * z$vw * (1+z$return) #Returns * the weights 
      tot_ret[i]  = sum(ret) # Sum it all up to reinvest 
    }
  }
return(tot_ret) #returns the calculations out of the loop
}

##
## This section retrives a string of numbers (return per rebalancing period)
##

top_score = cumalative_ret_portfolio(top) # runs the script on top portfolio 
mid_score = cumalative_ret_portfolio(mid) # runs on bot portf.
bot_score = cumalative_ret_portfolio(bot) # runs on bot portf.
```

```{r, WML weighting and calculations}
#creating wml portfolio
wml_weights= function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  temp_holder = 0 %>% as.data.frame()
  z = 0 
  count = 0 
  pb = txtProgressBar(min = 0, max = length(reb_month), initial = 0, style = 3) # progress bar
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == date) # Select the first date on the list, filter out the rest
      count = count + 1 # Adds 1 to the counter for progressbar 
      setTxtProgressBar(pb,count) # Starting the progress bar
          x = z
          tot_w = sum(x$mcap)  
          x$vw  = (x$mcap/tot_w) %>% as.data.frame()
          temp_holder = bind_rows(x,temp_holder) 
          if (i == 1) { # This is only here to remoe the one binding col!
             temp_holder = temp_holder %>% slice(-c(nrow(temp_holder)))
             } 
        }
      return(temp_holder)
    }
  
wml_cumalative_ret_portfolio = function (port){ # Defining function
reb_month =  unique(port$date) # finds each re-balancing months from the portfolio
tot_ret = c() # empty vector 
sum_w = 0
temp = 0 
  for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
    if (i == 1){  # First round do this 
      z = port %>% dplyr::filter(reb_month[i] == port$date) # Select the first date on the list, filter out the rest 
      #z = z %>% dplyr::filter(ret_inv != 0) 
      ret = (1+z$ret_inv) * z$vw  # Investing 1 dollar divided by stocks multiplied with returns
      tot_ret[i]  = sum(ret) # Summarize all the returns from investing one dollar 
    }
    if (i > 1){ # When its beyond start date, (t+1). Do the following
      z = port %>% dplyr::filter(reb_month[i] == port$date) # Filter out the dates not needed
      tot_w = sum(z$vw)
      ret = (tot_ret[i-1]/tot_w)* z$vw * (1+z$ret_inv) #Returns * the weights 
      tot_ret[i]  = sum(ret) # Sum it all up to reinvest 
    }
  }
return(tot_ret) #returns the calculations out of the loop
}

bot_1$ret_inv = bot$return * -1   # Inverting returns 
top_1$ret_inv = top$return        # just adding the same on top for easier merging
wml_score = rbind(top_1,bot_1)    # Binding the to together 

wml_score_parent  = wml_weights(wml_score) # finding weights for wml
wml_score = wml_cumalative_ret_portfolio(wml_score_parent)

dfx = wml_score_parent %>% dplyr::filter(date == "2010-12-31") # Checking that weights sum to one 
sum(dfx$vw) # OK if it sums to one
# dfx = NULL
```


## NB! WML has bad returns to work with, therefor later on we will use TOP\Botto do the dynamic weighting on

```{r, just renaming some cols, and merging tables}
##
# Takes the dates and merges then with the scores, making a more user friendly experience
##
scores = function (portfolio, parent_portfolio){ # makes function
  portfolio = portfolio %>% as_tibble() # converts the score into a tibble for easier processing 
  colnames(portfolio)[1] = "cum_rets"  # Rename the cumalative returns 
  dates = parent_portfolio$date  %>% as.Date() %>% unique() # Find the dates from the parent calulations (pre last function)
  portfolio$date  =  dates  # takes the dates and put them into tbe portfolio
  return(portfolio) # Returns resaults 

} 

wml_score_parent = wml_score_parent %>% dplyr::select(all_of(selecter))

top_score_reb = scores(top_score, top) #Calls the function on the score and the parrent 
mid_score_reb = scores(mid_score, mid) # same but for mid score 
bot_score_reb = scores(bot_score, bot) # same but for bot score  bot_score_inverse
wml_score_reb = scores(wml_score, wml_score_parent)
```

```{r, combinding all tables from above baed on date}
# Combinding the data from above into one DF, for easier access 

total_table = function(){ 
  if ((top_score_reb$date == bot_score_reb$date) && (top_score_reb$date == bot_score_reb$date)) {
    total_score = NULL
    total_score$date  = as_tibble(top_score_reb$date)
    total_score$top = top_score_reb$cum_rets
    #total_score$mid = mid_score_reb$cum_rets # removed, no need to have it in
    total_score$bot = bot_score_reb$cum_rets
    total_score$wml = wml_score_reb$cum_rets
  return(total_score)
  }
}


total_score = as.data.frame(total_table()) # Saves the output as dataframe, so we can translate date to date
names(total_score)[names(total_score) == "value"] <- "date"
total_score[,1] = total_score$date  %>% as.Date() # Converts date to date format
```

```{r, importing fama franch data}
# Donwload research data from https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip
# Select monthly dataset. 
# Remember to scrub the data! Add Date as a column (or else it gets indexed) # Remove yearly data at the bottom 
# Remove notes top and bottom on the file, or else you will get strange data. 

ff_data = read.csv(file="F-F_Research_Data_Factors.CSV", sep=",", header = TRUE) # load file in working directory, seperation is , 
ff_data$date = ff_data$date %>%  ym()  #put the date into usable format in tune with our other datasets
ff_data = ff_data %>% dplyr::filter(date > portstart_date)  # Filter out everydate below 1990-09-01

day(ff_data$date) = days_in_month(ff_data$date) # Changing the day in our dataset to match the other data (ie last day of month)
ff_data$date = ff_data$date  %>% as.Date() 

###
### MATCHING DATES AGAIN TO MAKE SURE ITS IN ORDER!!!!! I HATE DATES !!!! 
day(total_score$date) = days_in_month(total_score$date) # to match up with ff_dates
day(total_score$date) = days_in_month(total_score$date)  
day(sp500$date) = days_in_month(sp500$date) 
```

```{r, converting FF data to percentages}
# Devide by 100 to get the percentage and not the "percentage" :) As in data 5.2 is turned into 0.052 to match data 
ff_data$ff_mkt_rf = ff_data$Mkt.RF/100
ff_data$ff_smb = ff_data$SMB/100
ff_data$ff_hml = ff_data$HML/100
ff_data$ff_rf = ff_data$RF/100
```

```{r, syncing the dates of ff data with other dates }
#
# Take data from ff_data, regarding risk free rate etc. Filter it so we can fit it into the calculated dataset
# Here we match up the dates from the dataset, to make sure our data is in its correct places 

locate_ffdates = function(data){
  uniq_dates = unique(total_score$date) 
  temp_var1 = NULL
  temp_var2 = NULL
  for (i in 1:length(uniq_dates)){   # Goes through all dates in total_score
    temp_var1   = data %>% dplyr::filter(date == uniq_dates[i])# filters out all the dates that are not in totalscore.
    temp_var2= bind_rows(temp_var1 ,temp_var2)
  }
  return(temp_var2) #Returns the data 
}

total_score$date = total_score$date %>% as.Date() # just making sure dates are in order as usual before i put it into the next func

#ff_data = ff_data %>% arrange(date) %>% filter(date <= total_score$date[1])
ff_dates = locate_ffdates(ff_data) #using the function on the FF data to get Risk free return and other data

sp500_dates = locate_ffdates(sp500)

selecter = c("date", "cum_ret") # Variables to have in top and bot tibble
sp500_dates2 = sp500_dates %>% ungroup() %>%  dplyr::select(all_of(selecter))
names(sp500_dates2)[2] = "sp500_cumret" 
  
total_score = merge(total_score, ff_dates, by.x="date", by.y="date") # Moving the dates into total_score df
total_score = merge(total_score, sp500_dates2, by.x="date", by.y="date")
```

## As previously mentioned the BOT performs very well. But this is due to the dotcom momentum bounce, then the 2008/2009 crash ! 

```{r, graphing the end returns}
# Visualization of the scores 
colors <- c("top" = "blue", "mid" = "red", "bot" = "orange", "wml" = "green" , " ff_mkt_rf" = "pink", " ff_rf" = "black")
# This top part is just to have a nice legend on the side of the graph 

total_score$date = total_score$date %>% as.Date()  # Make sure dates are DATES ! 

ggplot(total_score, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       #geom_line(aes(y = mid, color = "mid" ), size = 1) + 
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       geom_line(aes(y = wml, color = "wml"), size = 1) + 
       geom_line(aes(y = sp500_cumret, color = "black"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Portfolio Cumulative Returns') +
    scale_y_continuous(breaks = seq(0,10,1)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y',) +
    scale_color_manual(values = colors)

```

```{r, cumalative returns for the ff data}
# Producing cumalative returns for the FF data
total_score$cu_ff_mkt_rf =  cumprod(1+total_score$ff_mkt_rf)
total_score$cu_ff_smb =  cumprod(1+total_score$ff_smb)
total_score$cu_ff_hml =  cumprod(1+total_score$ff_hml)
total_score$cu_ff_rf =  cumprod(1+total_score$ff_rf)
```

```{r, plotting with some ff data}
# Just adding some more data to investigate 
# Do note that SP500 and cumulated fama french market and risk free rate is very similar.
colors <- c("top" = "blue", "bot" = "orange", "wml" = "green" , " cu_ff_mkt_rf" = "red", " sp500" = "black")
total_score$date = total_score$date %>% as.Date()  

ggplot(total_score, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       #geom_line(aes(y = mid, color = "mid"), size = 1) + 
       geom_line(aes(y = wml, color = "wml"), size = 1) + 
       geom_line(aes(y = cu_ff_mkt_rf, color = " cu_ff_mkt_rf"), size = 1) + 
       geom_line(aes(y = sp500_cumret, color = "sp500"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Portfolio Cumulative Returns') +
    scale_y_continuous(breaks = seq(0,10,1)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```

```{r, regression on wml}
# I belive this might be redundant, i coded this along time ago, now i dont want to remove it becouse something might happen(?)
#
total_score$return_wml_reg = total_score$wml - total_score$cu_ff_rf # 
total_score

#Regressing the returns
ff_reg = lm(return_wml_reg ~  ff_mkt_rf + ff_smb + ff_hml, data = total_score)
summary(ff_reg)
```

```{r, calculating returns for deciles and formating the tables}
#
# Before implementing a time series we need to split top and bot into monthly returns
#
month_ret = function (portfolio){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  tot_ret = c() # empty vector 
  sum_w = 0
  temp = 0 
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest 
      ret = (z$return)  # This is a bit redundant
      tot_ret[i]  = sum(ret) # Summarize all the returns in a month 
    }
return(tot_ret) #return the returns for each month out of the loop based o
}

month_ret_top = month_ret(top) %>% bind_cols(unique(top$date)) # Binding uniqe dates 
names(month_ret_top)[1] = "ret_mont_top" # Rename cols 
names(month_ret_top)[2] = "date"

month_ret_bot = month_ret(bot) %>% bind_cols(unique(bot$date))
names(month_ret_bot)[1] = "ret_mont_bot"
month_ret_tot = bind_cols(month_ret_top,month_ret_bot$ret_mont_bot)
names(month_ret_tot)[3] = "ret_mont_bot"

month_ret_tot = month_ret_tot %>% arrange(date)

month_ret_tot = bind_cols(month_ret_tot,sp500_dates$return)
names(month_ret_tot)[4] = "sp500_ret"

month_ret_tot = month_ret_tot %>% relocate(date, .before = ret_mont_top) # OCD kicking in, moving date to front col 

month_ret_tot %>% head()
```

```{r, calculating betas, making some time series for top and bot}
# Calculating betas

beta = cov(total_score$top, total_score$sp500_cumret)/var(total_score$sp500_cumret) # Beta calc test 

ts_sp500_ret = ts(sp500_dates$return, start = c(2005, 1), freq = 12) # Creating time series
ts_top_ret = ts(month_ret_tot$ret_mont_top , start = c(2005, 1), freq = 12) 
ts_bot_ret = ts(month_ret_tot$ret_mont_bot , start = c(2005, 1), freq = 12)


CAPM.beta(ts_top_ret, ts_sp500_ret, Rf = 0) # THIS Works in our test!  This is just a dev test to check if function works 

beta = cov(ts_top_ret, ts_sp500_ret)/var(ts_sp500_ret) # This is equal to the one above, to check the function! 
total_score$alpha = total_score$wml - total_score$ff_rf - beta * (total_score$ff_mkt_rf - total_score$ff_rf) # calculating alpha. Think this might be redundant 
```

#
# Here we make the rolling regression, But first we normalize it with the "normalize function" so we can get numbers between 0 and 1 
# This is becouse our dataset is very small, and we dont want to loose data. Additionaly we are investigating volatility so the numbers 
# We will use still has a ratio to find volatility

```{r, normalizing returns, and 6m rolling regression to get Betas}
library(rollRegres) # Using rolling regression 

normalizefunc = function(portfolio, colnumb){ # Making the normf. funk 
  x = portfolio[colnumb]
  y = (x-min(x))/(max(x)-min(x))
  return(y)
}

month_ret_tot$norm_ret_mont_top = normalizefunc(month_ret_tot, 2)  # Normalizing 
month_ret_tot$norm_ret_mont_bot = normalizefunc(month_ret_tot, 3)
month_ret_tot$norm_sp500_ret = normalizefunc(month_ret_tot, 4)

month_ret_tot$norm_ret_mont_top = as.numeric(as.character(unlist(month_ret_tot$norm_ret_mont_top))) # Unlisting etc becouse we need to 
month_ret_tot$norm_ret_mont_bot = as.numeric(as.character(unlist(month_ret_tot$norm_ret_mont_bot))) # Or else we have problems regressing
month_ret_tot$norm_sp500_ret = as.numeric(as.character(unlist(month_ret_tot$norm_sp500_ret)))

month_ret_tot[sapply(month_ret_tot, is.infinite)] = 0.99999 # Dummy encoding since log cant be 0 or 1 we do this 
month_ret_tot[month_ret_tot == 0] <- 0.000001 # Dummy encoding
month_ret_tot[month_ret_tot == 1] <- 0.999999 # Dummy encoding

reg_top = roll_regres(norm_ret_mont_top ~ norm_sp500_ret, month_ret_tot, width = 6, #Beta top vs SP500, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))

reg_bot = roll_regres(norm_ret_mont_bot ~ norm_sp500_ret, month_ret_tot, width = 6, #Beta top vs SP500, 6 month window
            do_compute = c("sigmas", "r.squareds", "1_step_forecasts"))


reg_top_sum = bind_cols(month_ret_tot$date ,as.data.frame(reg_top$coefs)) # Binding it together  in a new df 
reg_bot_sum = bind_cols(month_ret_tot$date ,as.data.frame(reg_bot$coefs)) 

names(month_ret_tot)[5] = "norm_ret_mont_top" # Renaming cols 
names(month_ret_tot)[6] = "norm_ret_mont_bot"
names(month_ret_tot)[7] = "norm_ret_sp500"

names(reg_top_sum)[1] = "date" #renaming cols 
names(reg_top_sum)[3] = "beta_top"

names(reg_bot_sum)[1] = "date" # Guess what? We are renaming cols 
names(reg_bot_sum)[3] = "beta_bot"

reg_top_sum = reg_top_sum %>% na.omit() # omits NAs that might have been produced (it should not be any though)
reg_bot_sum = reg_bot_sum %>% na.omit() # Just to be on the safe side 

reg_top_sum$date = reg_top_sum$date %>% as.Date() # Make sure data is date 

devtools::unload("rollRegres") # Unload the pack 
```

# Now we have found the Betas, we wish to highlight the TOP and BOT performance under a crash.
# From the graph below we can clearly see bot is providing significantly better resaults then top during the "bounce back" 
# after a momentum crash

```{r, plotting log normalized beta}
# Choose from and too dates here to zoom in on graph
from_date = "2008-12-30" 
to_date = "2011-01-01"
##
##

filtered_reg_top_sum = reg_top_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Using filter to zoom
filtered_reg_bot_sum = reg_bot_sum %>% dplyr::filter(date > from_date) %>% dplyr::filter(date < to_date) # Filter to zoom 


colors <- c("top" = "blue", "bot" = "orange") # Getting the nice legend on the side, and choose colours 
ggplot(filtered_reg_top_sum, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = beta_top, color = "top"), size = 0.75) +
       geom_line(data = filtered_reg_bot_sum, aes(y = beta_bot, color = "bot"), size = 0.75) +
       labs(x = 'Date',
       y = '6 month rolling log beta ',
       title = 'Top and bot betas') +
    scale_y_continuous(breaks = seq(-5,5,1)) + # Range of messurements 
    scale_x_date(date_breaks = '1 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```

```{r, plotting alpha}
ggplot(data=total_score,aes(x=date)) + geom_line(aes(y=alpha, color="alpha"),colour="#009682")+
  xlab("Date")+
  ylab("Alpha")
```

```{r, Alpa messurements }
mean(total_score$alpha) # Investigating alphas 
count(total_score$alpha > 0)
count(total_score$alpha < 0)
```


# This is the US reccession indicator. This is just here to see that it plots correctly, as we will use it in some graphs later on 

```{r, bearmarket indicator}
# Recession indicators retrived from https://fred.stlouisfed.org/series/USREC

rec_ind = read.csv("USREC.csv") #load 

rec_ind$date= rec_ind$DATE %>% as.Date() # dates are dates 
rec_ind= dplyr::select(rec_ind, c(date, USREC)) # select what we need 

day(rec_ind$date)<-days_in_month(rec_ind$date) #find last day in months, and set dates to our standard dates 

dates_rec_ind = locate_ffdates(rec_ind) #using the function from the FF data bind our dataset to the correct values 
  
total_score = merge(total_score, dates_rec_ind, by.x="date", by.y="date") # Merges this FF dataset with Maindataset 

total_score$bear_market = total_score$USREC*6.5 # Just multiplying the 1 so its easier to see on a graph 

ggplot(data=total_score,aes(x=date)) + # Plotting 
            geom_area(aes(y=USREC,color="red"),fill=rgb(red = 1, green = 0, blue = 0, alpha = 0.5))+
            geom_line(aes(y=top, color="top"))+
            xlab("Date")+
            ylab("Dollar value of investment")+
            ylim(0,6.5)

```

#
# We test for stationarity white noise etc 

```{r, testing top for stationarity, white noise auto regression}
library(aTSA)
library(forecast)

# from calculating betas we have the ts from top bot and s&p

testing_ts = function(timeseries, lag) {
  plot(timeseries)
  plot(timeseries^2)
  acf(timeseries, lag.max=lag) #setting the lag, checking for autocorrelation
  adf.test(timeseries) # p-value > 0 .05 therefor we reject the hypothesis that it is nonstationary. Therefor it is stationary.
  pacf(timeseries)
  Box.test(timeseries, lag = lag, type = "Ljung") # White noise test 
  decompose_ts_top = decompose(ts_top_ret) # Decomposing the inputs 
  decompose_ts_top %>% plot() #plotting when done 
}

decompose_ts_top = diff(ts_top_ret, differences = 1)  # creating a first order decomposition of the time series
testing_ts(decompose_ts_top,6)
lambda = BoxCox.lambda(decompose_ts_top) # lambda, this is here from a previous code, (might be nice to have around )

#auto.arima(decompose_ts_top,D=1, approximation = FALSE, lambda = lambda, parallel = TRUE, stepwise=FALSE, num.cores = 30 ) 
# auto arima is ussed to pinpoint what parameters to use later 
```

```{r}
library(fGarch) # Testing the fgarch 

ts_top_garch = garchFit(~aparch(1,1), data=decompose_ts_top, trace=F, delta = 2, include.delta = F)
summary(ts_top_garch)
```

```{r, volatility calculations}
# Calculating volatility for our portfolios 

vol_calc = function (data){
  out = volatility(data, n = 6, calc = "close", N = 12, mean0 = FALSE)
  return(out)
}

month_ret_tot$top_vol = vol_calc(month_ret_tot$norm_ret_mont_top)
month_ret_tot$sp500_vol =  vol_calc(month_ret_tot$norm_ret_sp500)
month_ret_tot$bot_vol = vol_calc(month_ret_tot$norm_ret_mont_bot)

```

```{r, making time-series of sp500 for testing}
# Some of the functions require us to use timeseries, therefor we convert them here 

temp_sp500_ret = month_ret_tot %>% dplyr::select("sp500_ret","date") %>% arrange(date)
temp_close_sp = sp500_dates  %>% ungroup() %>% dplyr::select("close","date") %>% arrange(date)
temp_sp500_rvol = month_ret_tot %>%  dplyr::select("sp500_vol","date") %>% arrange(date)

row.names(temp_sp500_rvol) = temp_sp500_rvol$date
row.names(temp_close_sp) = temp_close_sp$date
row.names(temp_sp500_ret) = temp_sp500_ret$date

# temp_close_sp = temp_close_sp %>% dplyr::select(-c("date"))
# temp_sp500_ret = temp_sp500 %>% dplyr::select(-c("date")) # Used for bugfixing, removing the date column 
# temp_sp500_rvol = temp_sp500_rvol %>% dplyr::select(-c("date"))

ts_sp500_close = ts(temp_close_sp[,1], start = c(2000, 10), freq = 12) # closing prices
ts_sp500_ret = ts(temp_sp500_ret[,1], start = c(2000, 10), freq = 12) # returns  
ts_sp500_rvol = ts(temp_sp500_rvol[,1], start = c(2000, 10), freq = 12) # real volatility
```


```{r}
library(rugarch)
# Importing our garch model to fit the data on, in this case we use the return of top portfolio 


temp_top_ret = month_ret_tot %>%  dplyr::select("ret_mont_top","date") %>% arrange(date) # Ordering 
ts_temp_top_ret = ts(temp_top_ret[,1], start = c(2000, 10), freq = 12) # Making time series 


garchspec = ugarchspec(mean.model = list(armaOrder = c(0,0)), # Setting the specs for our GJR-Garch model
                        variance.model = list(model = "gjrGARCH"), 
                        distribution.model = "sstd")


garchfit = rugarch::ugarchfit(data = ts_temp_top_ret  , spec = garchspec) # Estimating the model 

garchvol = sigma(garchfit)# Use the method sigma to retrieve the estimated volatilities 


plot(garchvol) # Plot the volatility

sqrt(uncvariance(garchfit)) # THis will be a basis for our manual tweaking later, as i was not able to implement the dynamic garch. And i worked 3 days on this.

devtools::unload("rugarch") # Dropping the pack
```

# 
# This section has been modified, this was done since i could not implement the dynamic gjr weighting. 
# The value border was based on the prevoius from garchvolatility, then it was refined manualy. 
# 
```{r, cumalative dynamic portfolio}
#Here we we run for each month, if the volatility is higher then 2.22 we invest in bot portfolio and not top, then we take those returns 
#next month and we reinvest them in another portfolio (rebalancing )

cumalative_ret_dynamic = function (portfolio,portfolio2, gjr_vol){ # Defining function
  reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
  tot_ret = c() # empty vector 
  border = 2.22 # Selecting our volitility cap 
    for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
      if (i == 1){  
        if (gjr_vol[i,1] >= border){ # High volatility invest in bot portfolio 
          x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date) # filters dates to get right portf 
          ret_x = (1+x$return) * x$vw # invests on the different stocks value weighted 
          tot_ret[i] = sum(ret_x) # sums it up so we can reinvest it next month 
        } 
        else {
          z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) 
          ret_z = (1+z$return) * z$vw # If garch vol is low invest in top portf 
          tot_ret[i] = sum(ret_z)
              }
                }
      if (i > 1){ 
          if (gjr_vol[i,1] <= border){ # we do the same as above, with minor tweaks 
            x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date)
            ret_x = tot_ret[i-1] * (1+x$return) * x$vw
            tot_ret[i] = sum(ret_x)
          }
          else{
            z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date)
            ret_z = tot_ret[i-1] * (1+z$return) * z$vw
            tot_ret[i] = sum(ret_z)
          }
        }
    }
  return(tot_ret) #returns the calculations out of the loop
  }


top_2 = top_1 %>% dplyr::filter(date > "2000-10-01") # Date selection of portfolio is just here for testing 
bot_2 = bot_1 %>% dplyr::filter(date > "2000-10-01")
garchvol1 = garchvol %>% as.data.frame() # This is just fitting the garchvol to our dataset 
garchvol1 = cbind(garchvol1, total_score) 
colnames(garchvol1)[1] = "gjr_vol" 
rownames(garchvol1)=NULL
garchvol1 = garchvol1 %>% arrange(date) %>% dplyr::select(c("gjr_vol","date"))
garchvol1 = garchvol1 %>% dplyr::filter(date > "2000-10-01")

top_dynamic = cumalative_ret_dynamic(top_1,bot_1,garchvol1) # runs the script


top_dynamic %>% plot(type = "l") # fast plot to see outcome 

arrange(total_score,date) # Just making sure dates are in order before we move dynamic resaults into date 
tot_score_final = cbind(total_score, top_dynamic) # binding it to our main table 

tot_score_final %>% tail() # Checking data 
```


#Here we plot the main findings. 
Here we can clearly see our "dynamic" is doing very well. HOWEVER, after investigating the data, we can see this performance is due the cumulative returns. This can partly be explained by the 2000s dotcom bubble, dynamic gains extreme momentum at the start, as we would be in bot port then we switch to top (changing starting dates to 2005 yields a very different graph!). We also have a bias between the monitor and keyboard, as I tuned the "border" variable to give us a nicer graph. 

Another finding is how "hard" the "dynamic" droped in 2009 as it would have higher volatility triggering the allocation to bot portfolio, therefor it most likely has the same shape as bot in the same timeframe. 

```{r}
colors = c("top" = "blue", "bot" = "orange", "dynamic" = "green", "sp500" = "red", "USREC" = "purple")

ggplot(total_score, aes(x = date)) + # Uses main dataset
       geom_line(aes(y = top, color = "top"), size = 1) +
       geom_line(aes(y = bot, color = "bot"), size = 1) + 
       geom_line(aes(y = top_dynamic, color = "dynamic"), size = 1) + 
       geom_line(aes(y = sp500_cumret, color = "sp500"), size = 1)+
        geom_line(aes(y = USREC/3, color = "USREC"), size = 1)+
       labs(x = 'Date',
       y = 'Cumulative Returns',
       title = 'Portfolio Cumulative Returns') +
    scale_y_continuous(breaks = seq(0,15,1)) +
    scale_x_date(date_breaks = '5 year',
               date_labels = '%Y') +
    scale_color_manual(values = colors)
```

```{r}

x = cbind(month_ret_tot$date,month_ret_tot$ret_mont_top) %>% as.data.frame()
x$V1 = x$V1 %>% as.Date()
x = x %>% arrange(V1)
y = x$V2 


x$norm_sp = (y-min(y))/(max(y)-min(y)) 
#SP.wml = SP.wml[3]

#x = x %>% arrange(V1)
row.names(x) = x$V1


#sp_test = cbind(SP.wml, DAXConst$X)
#rownames(sp_test) = sp_test[[2]]
#sp_test = sp_test[1:1]
x = x#[72:nrow(x),]
ts_sp_test = ts(x[3], start = c(2000,10), frequency = 12 )

garchspec <- ugarchspec(mean.model = list(armaOrder = c(0, 0), archm = TRUE, archpow = 2), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")

cl = makePSOCKcluster(25)

gjrgarchroll <- ugarchroll(garchspec, data = ts_sp_test, n.start = 12,
                        refit.window = "moving", 
                        refit.every = 6,
                        cluster = cl,)  # initates cluster computing)
show(ugarch_roll)
stopCluster(cl)
plot(ugarch_roll)

garchfit <- ugarchfit(data = ts_sp_test, spec = garchspec)
round(coef(garchfit)[1:2], 4)
plot(fitted(garchfit))
```


################
################ Works to down here. Cant get dynamic to work like it should.
################

```{r}
library(parallel)

#temp_top_ret = month_ret_tot %>%  dplyr::select("ret_mont_top","date") %>% arrange(date)
#ts_temp_top_ret = ts(temp_top_ret[,1], start = c(2000, 10), freq = 12)

temp_top_ret = month_ret_tot %>%  dplyr::select("ret_mont_top","date") %>% arrange(date)
#temp_top_ret = temp_top_ret[,1] / 100
ts_temp_top_ret = ts(temp_top_ret[1:100,1] /100, start = c(2000, 10), freq = 12)

temp_norm_top_ret = month_ret_tot %>%  dplyr::select("norm_ret_mont_top","date") %>% arrange(date)
temp_norm_top_ret = temp_norm_top_ret %>% log()
temp_norm_top_ret = ts(temp_norm_top_ret[1:100,1] /100, start = c(2000, 10), freq = 12)



garchspec = ugarchspec(mean.model = list(armaOrder = c(0,0)),
                       variance.model = list(model = "gjrGARCH"),
                       garchOrder = c(1,1),
                       distribution.model = "sstd")

cl = makePSOCKcluster(25)

ugarch_roll = rugarch::ugarchroll(spec = garchspec, # garchspec model 
                                  data = temp_norm_top_ret, #inn data 
                                  n.start = 1, # when to start wtesting
                                  forecast.length = 1, #how far to forcast ( idealy this should be a trading month in days.) But we have 1 month instead
                                  refit.every = 6, # How often to refit (6 months is from the paper (or 126 trading days))
                                  refit.window = "moving", # Moving window to test 
                                  solver = "hybrid",  # Solver to use 
                                  calculate.VaR = TRUE,  # Calculate Value at risk or not 
                                  solver.control = list(tol = 1e-12),   # Solver controll 
                                  VaR.alpha = 0.19,  # 19 % var from paper 
                                  cluster = cl,  # initates cluster computing
                                  keep.coef = TRUE # keeping the coeficients 
                                  )

show(ugarch_roll)
stopCluster(cl)
plot(ugarch_roll)

# nå kjører vi regresion mellom kalkulert volatility og GJR volatility


predic = as.data.frame(ugarch_roll)

sigma_reg = lm(predic$Sigma ~  temp_norm_top_ret )

month_ret_tot$top_vol[61:nrow(month_ret_tot)] %>% plot(type = "l")

#ff_reg = lm(return_wml_reg ~  ff_mkt_rf + ff_smb + ff_hml, data = total_score)
weights_pred = lm(predic$Realized ~  predic$Sigma)

error = predic$Realized - predic$Mu
div = error ^ 2 - predic$Sigma ^ 2
mean(div^2)

x = sqrt(12)*ugarch_roll@forecast$density$Sigma

an_volatility = 0.15 # 15% of anualized risk
w = an_volatility/x # weights assigned to risky asset 

plot(x, type = "l")

#ts_spp = 1.035186 * apply(fitted(sim),2,'cumsum') + 1.035186 # calculating the change in price
#matplot(p, type = "l", lwd = 3)

report(ugarch_roll, typ = "VaR", VaR.alpha = 0.19, conf.level = 0.99)
```

```{r}
library(fGarch)

Fit01 =  garchFit(~ garch(1,1), data=temp_norm_top_ret, cond.dist="norm", include.mean = FALSE, trace = FALSE)
Fit01

Fit01@fit$ics

res1  = ts_temp_top_ret/Fit01@sigma.t  

plot(ts_temp_top_ret)
plot(Fit01@sigma.t, type="l") 

ts_1_temp_top_ret = ts_temp_top_ret %>% as.xts()

sigma.upper = xts(1.96*Fit01@sigma.t, order.by=index(ts_1_temp_top_ret))
sigma.lower = xts(-1.96*Fit01@sigma.t, order.by=index(ts_1_temp_top_ret))
    
plot( cbind(ts_temp_top_ret, sigma.upper, sigma.lower),
    col=c("black", "red", "red"), lwd=c(2,1,1) )

plot(  as.numeric(ts_1_temp_top_ret["2000::"]), type="h", lwd=2)
lines( as.numeric(sigma.upper["2000::"]), col="red")
lines( as.numeric(sigma.lower["2000::"]), col="red")
```

<!-- # ```{r} -->
<!-- # library(Hmisc) -->
<!-- # # en timeseries frem til 2008 -->
<!-- # -->
<!-- # temp_top_ret = month_ret_tot %>%  dplyr::select("ret_mont_top","date") %>% arrange(date) -->
<!-- # #temp_top_ret = temp_top_ret[,1] / 100 -->
<!-- # ts_temp_top_ret = ts(temp_top_ret[1:80,1] /100, start = c(2000, 10), freq = 12) -->
<!-- # -->
<!-- # # Så fitter man modellen til en GJR modell, lektyren anbefaler 1,1 garch -->
<!-- # -->
<!-- # garch_sp500_spec = ugarchspec(mean.model = list(armaOrder = c(0,0)), -->
<!-- #                    variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), -->
<!-- #                    distribution.model = 'sstd') -->
<!-- # -->
<!-- # garch_top_fit_close = ugarchfit(garch_sp500_spec, temp_norm_top_ret) -->
<!-- # -->
<!-- # # Så gjør vi bare det i 2008 på predictions -->
<!-- # forcast_gjr_top = ugarchforecast(fitORspec = garch_top_fit_close, n.ahead = 12) -->
<!-- # -->
<!-- # mu. <- fitted(garch_top_fit_close) -->
<!-- # sig. <- sigma(garch_top_fit_close) -->
<!-- # -->
<!-- # -->
<!-- # # Plot forecast -->
<!-- # plot(forcast_gjr_top, which=1) -->
<!-- # plot(forcast_gjr_top, which=3) -->
<!-- # -->
<!-- # -->
<!-- # #jo Høyere volatility predcition jo høyere investering MEN BARE I NEDE TIDEN -->
<!-- # forcast_gjr_top %>% plot() -->
<!-- # ``` -->


## Chris stuff ------------------------------------------------- IDK what this is???

```{r, 6 month forwardlooking returns?}
# This creates 12 cols, these cols will tell you if you sold it today, what will be the return 

ret_1_6 = function (portfolio){ # Defining function
  uniq_stocks =  unique(portfolio$symbol) # finds each stock to calc returns
  temp_holder = 0 %>% as.data.frame()
  temp_holder2 = 0 %>% as.data.frame()
  temp_holder3 = 0 %>% as.data.frame()
  count = 0
  pb = txtProgressBar(min = 0, max = nrow(portfolio), initial = 0, style = 3) # progress bar
   z = 0 %>% as.data.frame()
  for (i in 1:nrow(portfolio)){ # for each date we have in the portfolio do this
      z = portfolio[i,]
      count = count + 1 # Adds 1 to the counter for progressbar 
      setTxtProgressBar(pb,count) # Starting the progress bar
      for(j in 1:6) {
        #temp_holder  = momentum(z$return, n=j)
        temp_holder = portfolio$wml[6+j+count]
        name = paste0("mom", sep = "_", j)
        z[[name]] =  temp_holder 
        if (j == 6){
          temp_holder2  = bind_rows(temp_holder2, z)
        } 
     }
    }
  return(temp_holder2)
}  

test  = ret_1_6(total_score)
test = test %>% dplyr::distinct(date, .keep_all = T)

wml_chris = test %>% slice(-c(1)) # Due to bind with one Zero, there is a redundant row, this is removed here. 
test = NULL 
```

```{r, forcast calculations}
# Return forcast ! 

wml_chris$forcast = (wml_chris$mom_6 / wml_chris$mom_1)^(1/6) 

wml_chris$forcast = ((wml_chris$forcast - 1)*100) # n[t-1]

wml_chris$forcast_2 = ((((wml_chris$forcast)^2 - wml_chris$forcast)^2)) #sigma^2[t-1]

wml_chris %>% head()
```

```{r, volatility calculations}

vol_calc = function (data){
  out = volatility(data, n = 6, calc = "close", N = 12, mean0 = FALSE)
  return(out)
}

month_ret_tot$top_vol = vol_calc(month_ret_tot$norm_ret_mont_top)
month_ret_tot$sp500_vol =  vol_calc(month_ret_tot$norm_ret_sp500)
month_ret_tot$bot_vol = vol_calc(month_ret_tot$norm_ret_mont_bot)

```

```{r, forcasting????}
### Now we can get the lambda

month_ret_tot$lambda = month_ret_tot$sp500_vol / sqrt(wml_chris$forcast_2) %>% as_tibble()

#### Now we can finaly get the weights 

wml_chris$weights_forcast = (1/month_ret_tot$lambda) * (wml_chris$forcast / wml_chris$forcast_2)
wml_chris$weights_forcast

```



# ```{r}
# # da må vi gjøre det slik 
# # Når den er under 2.14 går alt til top, ved økning til 3 legger man det til 
# 
# cumalative_ret_dynamic = function (portfolio,portfolio2, gjr_vol){ # Defining function
#   reb_month =  unique(portfolio$date) # finds each re-balancing months from the portfolio
#   tot_ret = c() # empty vector 
#   vt_weights = vt_weights %>% as_tibble()
#   start_value = 1 # as of 1 USD 
#   top_start_val = start_value * vt_weights[1,1] #%>% as_tibble()
#   #bot_start_val = start_value - top_start_val# %>% as_tibble()
#   #bot_start_val = start_value * 1-vt_weights[1] 
#     for (i in 1:length(reb_month)){ # for each date we have in the portfolio do this 
#       if (i == 1){  # First round do this 
#         z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest
#         x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date)
#         ret_z = top_start_val[1,1] * (1+z$return) * z$vw  
#         ret_x = bot_start_val[1,1] * (1+x$return) * x$vw
#         tot_ret[i]  = sum(ret_z, ret_x) 
#       }
#       if (i > 1){ # When its beyond start date, (t+1). Do the following
#         z = portfolio %>% dplyr::filter(reb_month[i] == portfolio$date) # Select the first date on the list, filter out the rest
#         x = portfolio2 %>% dplyr::filter(reb_month[i] == portfolio2$date)
#         ret_weights_z = vt_weights[i,1] * tot_ret[i-1] 
#         ret_weights_x = 1-vt_weights[i,1] * tot_ret[i-1]
#         ret_z = ret_weights_z[1,1] * z$vw * (1+z$return)  
#         ret_x = ret_weights_x[1,1] * x$vw * (1+x$return)
#         tot_ret[i]  = sum(ret_z, ret_x) # Sum it all up to reinvest 
#     }
#   }
# return(tot_ret) #returns the calculations out of the loop
# }
# 
# library(timetk)
# 
# top_2 = top_1 %>% dplyr::filter(date > "2008-10-01")
# bot_2 = bot_1 %>% dplyr::filter(date > "2008-10-01")
# garchvol1 = garchvol %>% as.data.frame()
# garchvol1 = cbind(garchvol1, total_score)
# colnames(garchvol1)[1] = "gjr_vol" 
# garchvol1 = garchvol1 %>% dplyr::select(c("gjr_vol","date"))
# garchvol1 = garchvol1 %>% dplyr::filter(date > "2008-10-01")
# 
# gjr_vol = garchvol1 
# 
# top_dynamic = cumalative_ret_dynamic(top_1,bot_1,garchvol1) # runs the script on top portfolio 
# #mid_score = cumalative_ret_portfolio(mid) # runs on bot portf.
# #bot_score = cumalative_ret_portfolio(bot) # runs on bot portf.
# 
# top_dynamic %>% plot(type = "line")
# ```


#
# This code was experimental, but has been comented out becouse i could not get it to work, but i feel i was realy close

<!-- ```{r} -->
<!-- library(quantmod) -->
<!-- library(xts) -->
<!-- library(PerformanceAnalytics) -->
<!-- library(rugarch) -->
<!-- library(ggplot2) -->



<!-- garch_sp500_spec = ugarchspec(mean.model = list(armaOrder = c(0,0)), -->
<!--                    variance.model = list(model = "gjrGARCH"), -->
<!--                    distribution.model = 'sstd') -->

<!-- garch_sp500_fit_close = ugarchfit(garch_sp500_spec, ts_sp500_close) -->

<!-- coef(garch_sp500_fit_close) -->
<!-- #garch_top_model = ugarchfit(data = ts_sp500_ret, spec = garch_top_spec, cond.dist="std" ) -->

<!-- vol_sp500_garch = ts(garch_sp500_fit_close@fit$sigma^2,start=c(2000,10),end=c(2010,12),frequency = 12) -->

<!-- plot(vol_sp500_garch,xlab="",ylab="",main="garch_sp500_fit_close") -->

<!-- plot(garch_sp500_fit_close, which = "all") -->

<!-- x = sqrt(12) * garch_sp500_fit_close@fit$sigma -->
<!-- plot(x,type="l") -->

<!-- y = ts_sp500_rvol -->
<!-- plot(y,type="l") -->

<!-- #plot(merge(x,y), multi.panel = TRUE, type = "l") -->

<!-- forcast_gjr_top = ugarchforecast(fitORspec = garch_sp500_fit_close, n.ahead = 12) -->
<!-- forcast_gjr_top -->

<!-- ############## NEW TEST GARCH ###################### -->

<!-- plot(forcast_gjr_top, which = "all" ) -->
<!-- #plot(garch_top_model) -->

<!-- forcast_gjr_top = ugarchforecast(fitORspec = forcast_gjr_top, n.ahead = 12) -->

<!-- plot(fitted(forcast_gjr_top)) -->
<!-- plot(sigma(forcast_gjr_top)) # from this we know we will have higher predicted volatility -->

<!-- x = sqrt(12)*sigma(forcast_gjr_top) -->

<!-- an_volatility = 0.15 # 15% of anualized risk -->
<!-- w = an_volatility/x # weights assigned to risky asset  -->

<!-- plot(merge(x,w), multi.panel = TRUE) -->
<!-- plot(x) -->
<!-- plot(w) -->


<!-- plot.zoo(fitted(sim)) -->
<!-- plot.zoo(sigma(sim)) -->

<!-- tail(ts_sp500_ret) -->

<!-- p = 1.035186 * apply(fitted(sim),2,'cumsum') + 1.035186 # calculating the change in price -->
<!-- matplot(p, type = "l", lwd = 3) -->
<!-- ``` -->
# ```{r}
# mean_sp500 = mean(ts_sp500_ret)
# errors = ts_sp500_ret - mean_sp500
# 
# par(mfrow = c(2,1),mar = c(3, 2, 2, 2)) # ??
# plot(abs(errors))
# 
# plot(abs(e))
# acf(abs(errors))
# 
# error2 = errors^2 
# ```


